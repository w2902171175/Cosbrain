# project/routers/knowledge/intelligent_recommendation.py
"""
æ™ºèƒ½æ¨èæ¨¡å— - åŸºäºç”¨æˆ·è¡Œä¸ºçš„æ™ºèƒ½æ¨èç³»ç»Ÿ
æä¾›ä¸ªæ€§åŒ–å†…å®¹æ¨èã€ç›¸å…³æ–‡æ¡£å‘ç°ã€æ™ºèƒ½æ ‡ç­¾å»ºè®®ç­‰åŠŸèƒ½
"""

import asyncio
import json
import uuid
import time
import math
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime, timedelta
from enum import Enum
from dataclasses import dataclass, asdict
import logging
from collections import defaultdict, Counter
import redis
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import jieba
import jieba.analyse
from sqlalchemy.orm import Session
from sqlalchemy import func, desc, and_, or_

logger = logging.getLogger(__name__)

class UserAction(str, Enum):
    """ç”¨æˆ·è¡Œä¸ºç±»å‹"""
    VIEW = "view"
    DOWNLOAD = "download"
    SEARCH = "search"
    BOOKMARK = "bookmark"
    SHARE = "share"
    COMMENT = "comment"
    RATE = "rate"
    EDIT = "edit"
    DELETE = "delete"
    UPLOAD = "upload"
    TAG = "tag"

class RecommendationType(str, Enum):
    """æ¨èç±»å‹"""
    CONTENT_BASED = "content_based"      # åŸºäºå†…å®¹
    COLLABORATIVE = "collaborative"      # ååŒè¿‡æ»¤
    BEHAVIOR_BASED = "behavior_based"   # åŸºäºè¡Œä¸º
    HYBRID = "hybrid"                   # æ··åˆæ¨è
    TRENDING = "trending"               # çƒ­é—¨æ¨è
    SIMILAR_USERS = "similar_users"     # ç›¸ä¼¼ç”¨æˆ·

@dataclass
class UserBehavior:
    """ç”¨æˆ·è¡Œä¸ºè®°å½•"""
    user_id: int
    action: UserAction
    document_id: int
    kb_id: int
    timestamp: datetime
    session_id: str
    metadata: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'user_id': self.user_id,
            'action': self.action,
            'document_id': self.document_id,
            'kb_id': self.kb_id,
            'timestamp': self.timestamp.isoformat(),
            'session_id': self.session_id,
            'metadata': self.metadata or {}
        }

@dataclass
class UserProfile:
    """ç”¨æˆ·ç”»åƒ"""
    user_id: int
    interests: Dict[str, float]        # å…´è¶£æ ‡ç­¾åŠæƒé‡
    categories: Dict[str, float]       # å†…å®¹ç±»åˆ«åå¥½
    activity_level: float              # æ´»è·ƒåº¦
    preferred_formats: List[str]       # åå¥½æ ¼å¼
    interaction_patterns: Dict[str, Any]  # äº¤äº’æ¨¡å¼
    last_updated: datetime
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class Recommendation:
    """æ¨èç»“æœ"""
    document_id: int
    score: float
    reason: str
    rec_type: RecommendationType
    metadata: Dict[str, Any] = None

class BehaviorAnalyzer:
    """ç”¨æˆ·è¡Œä¸ºåˆ†æå™¨"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.behavior_weights = {
            UserAction.VIEW: 1.0,
            UserAction.DOWNLOAD: 3.0,
            UserAction.SEARCH: 0.5,
            UserAction.BOOKMARK: 2.5,
            UserAction.SHARE: 2.0,
            UserAction.COMMENT: 1.5,
            UserAction.RATE: 2.0,
            UserAction.EDIT: 1.0,
            UserAction.DELETE: -1.0,
            UserAction.UPLOAD: 1.0,
            UserAction.TAG: 1.0
        }
        
    async def record_behavior(self, behavior: UserBehavior):
        """è®°å½•ç”¨æˆ·è¡Œä¸º"""
        # ä¿å­˜åˆ°Redis
        behavior_key = f"behavior:{behavior.user_id}:{behavior.timestamp.strftime('%Y%m%d')}"
        
        await self.redis_client.lpush(
            behavior_key,
            json.dumps(behavior.to_dict())
        )
        
        # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆä¿ç•™90å¤©ï¼‰
        await self.redis_client.expire(behavior_key, 90 * 24 * 3600)
        
        # æ›´æ–°å®æ—¶è¡Œä¸ºç»Ÿè®¡
        await self._update_realtime_stats(behavior)
        
    async def _update_realtime_stats(self, behavior: UserBehavior):
        """æ›´æ–°å®æ—¶è¡Œä¸ºç»Ÿè®¡"""
        today = datetime.now().strftime('%Y%m%d')
        
        # ç”¨æˆ·ä»Šæ—¥è¡Œä¸ºè®¡æ•°
        user_stats_key = f"stats:user:{behavior.user_id}:{today}"
        await self.redis_client.hincrby(user_stats_key, behavior.action, 1)
        await self.redis_client.expire(user_stats_key, 24 * 3600)
        
        # æ–‡æ¡£ä»Šæ—¥è®¿é—®è®¡æ•°
        doc_stats_key = f"stats:doc:{behavior.document_id}:{today}"
        await self.redis_client.hincrby(doc_stats_key, behavior.action, 1)
        await self.redis_client.expire(doc_stats_key, 24 * 3600)
        
        # å…¨å±€çƒ­é—¨ç»Ÿè®¡
        if behavior.action in [UserAction.VIEW, UserAction.DOWNLOAD]:
            await self.redis_client.zincrby(
                f"trending:{today}",
                self.behavior_weights[behavior.action],
                behavior.document_id
            )
            await self.redis_client.expire(f"trending:{today}", 24 * 3600)
            
    async def get_user_behaviors(self, user_id: int, days: int = 30) -> List[UserBehavior]:
        """è·å–ç”¨æˆ·è¡Œä¸ºå†å²"""
        behaviors = []
        
        for i in range(days):
            date = (datetime.now() - timedelta(days=i)).strftime('%Y%m%d')
            behavior_key = f"behavior:{user_id}:{date}"
            
            behavior_data = await self.redis_client.lrange(behavior_key, 0, -1)
            
            for data in behavior_data:
                try:
                    behavior_dict = json.loads(data)
                    behavior_dict['timestamp'] = datetime.fromisoformat(behavior_dict['timestamp'])
                    behaviors.append(UserBehavior(**behavior_dict))
                except Exception as e:
                    logger.error(f"è§£æè¡Œä¸ºæ•°æ®å¤±è´¥: {e}")
                    
        return sorted(behaviors, key=lambda x: x.timestamp, reverse=True)
        
    async def analyze_user_interests(self, user_id: int, db: Session) -> Dict[str, float]:
        """åˆ†æç”¨æˆ·å…´è¶£"""
        behaviors = await self.get_user_behaviors(user_id)
        
        if not behaviors:
            return {}
            
        # è·å–ç”¨æˆ·è®¿é—®çš„æ–‡æ¡£ä¿¡æ¯
        doc_ids = list(set(b.document_id for b in behaviors))
        
        # è¿™é‡Œéœ€è¦ä»æ•°æ®åº“è·å–æ–‡æ¡£ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ ‡ç­¾ã€ç±»åˆ«ç­‰
        # ç®€åŒ–ç¤ºä¾‹
        interest_scores = defaultdict(float)
        
        for behavior in behaviors:
            weight = self.behavior_weights.get(behavior.action, 1.0)
            
            # æ ¹æ®æ—¶é—´è¡°å‡
            days_ago = (datetime.now() - behavior.timestamp).days
            time_decay = math.exp(-days_ago / 30.0)  # 30å¤©è¡°å‡å› å­
            
            score = weight * time_decay
            
            # è¿™é‡Œåº”è¯¥åŸºäºæ–‡æ¡£çš„å®é™…æ ‡ç­¾å’Œç±»åˆ«æ¥æ›´æ–°å…´è¶£åˆ†æ•°
            # ç®€åŒ–ç¤ºä¾‹ï¼šåŸºäºæ–‡æ¡£IDæ¨æ–­ç±»åˆ«
            category = f"category_{behavior.document_id % 10}"
            interest_scores[category] += score
            
        # å½’ä¸€åŒ–åˆ†æ•°
        if interest_scores:
            max_score = max(interest_scores.values())
            interest_scores = {k: v / max_score for k, v in interest_scores.items()}
            
        return dict(interest_scores)

class ContentAnalyzer:
    """å†…å®¹åˆ†æå™¨"""
    
    def __init__(self):
        self.tfidf_vectorizer = None
        self.doc_vectors = None
        self.doc_ids = []
        
    async def analyze_document_content(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£å†…å®¹"""
        if not documents:
            return {}
            
        # æå–æ–‡æ¡£æ–‡æœ¬
        texts = []
        doc_ids = []
        
        for doc in documents:
            text = doc.get('content', '') or doc.get('title', '')
            if text:
                # ä¸­æ–‡åˆ†è¯
                words = jieba.cut(text)
                processed_text = ' '.join(words)
                texts.append(processed_text)
                doc_ids.append(doc['id'])
                
        if not texts:
            return {}
            
        # TF-IDFå‘é‡åŒ–
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=['çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äº†', 'ä¹Ÿ', 'æˆ‘'],
            ngram_range=(1, 2)
        )
        
        self.doc_vectors = self.tfidf_vectorizer.fit_transform(texts)
        self.doc_ids = doc_ids
        
        # æå–å…³é”®è¯
        keywords = {}
        for i, text in enumerate(texts):
            doc_keywords = jieba.analyse.extract_tags(text, topK=10, withWeight=True)
            keywords[doc_ids[i]] = dict(doc_keywords)
            
        # æ–‡æ¡£èšç±»
        if len(texts) > 5:
            clusters = self._cluster_documents(self.doc_vectors)
            cluster_info = dict(zip(doc_ids, clusters))
        else:
            cluster_info = {}
            
        return {
            'keywords': keywords,
            'clusters': cluster_info,
            'vectorizer_vocabulary': len(self.tfidf_vectorizer.vocabulary_) if self.tfidf_vectorizer else 0
        }
        
    def _cluster_documents(self, vectors, n_clusters: int = None) -> List[int]:
        """æ–‡æ¡£èšç±»"""
        if n_clusters is None:
            n_clusters = min(5, max(2, vectors.shape[0] // 10))
            
        try:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            clusters = kmeans.fit_predict(vectors.toarray())
            return clusters.tolist()
        except Exception as e:
            logger.error(f"æ–‡æ¡£èšç±»å¤±è´¥: {e}")
            return [0] * vectors.shape[0]
            
    def calculate_content_similarity(self, doc_id1: int, doc_id2: int) -> float:
        """è®¡ç®—æ–‡æ¡£å†…å®¹ç›¸ä¼¼åº¦"""
        if not self.doc_vectors or not self.doc_ids:
            return 0.0
            
        try:
            idx1 = self.doc_ids.index(doc_id1)
            idx2 = self.doc_ids.index(doc_id2)
            
            vec1 = self.doc_vectors[idx1]
            vec2 = self.doc_vectors[idx2]
            
            similarity = cosine_similarity(vec1, vec2)[0][0]
            return float(similarity)
            
        except (ValueError, IndexError):
            return 0.0
            
    def find_similar_documents(self, doc_id: int, top_k: int = 5) -> List[Tuple[int, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ–‡æ¡£"""
        if not self.doc_vectors or not self.doc_ids:
            return []
            
        try:
            idx = self.doc_ids.index(doc_id)
            target_vector = self.doc_vectors[idx]
            
            similarities = cosine_similarity(target_vector, self.doc_vectors)[0]
            
            # æ’é™¤è‡ªèº«
            similarities[idx] = -1
            
            # è·å–æœ€ç›¸ä¼¼çš„æ–‡æ¡£
            similar_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for i in similar_indices:
                if similarities[i] > 0:
                    results.append((self.doc_ids[i], float(similarities[i])))
                    
            return results
            
        except (ValueError, IndexError):
            return []

class RecommendationEngine:
    """æ¨èå¼•æ“"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.behavior_analyzer = BehaviorAnalyzer(redis_client)
        self.content_analyzer = ContentAnalyzer()
        
    async def generate_recommendations(self, user_id: int, kb_id: int, db: Session,
                                     limit: int = 10) -> List[Recommendation]:
        """ç”Ÿæˆæ¨è"""
        recommendations = []
        
        # 1. åŸºäºå†…å®¹çš„æ¨è
        content_recs = await self._content_based_recommendations(user_id, kb_id, db, limit // 2)
        recommendations.extend(content_recs)
        
        # 2. åŸºäºè¡Œä¸ºçš„æ¨è
        behavior_recs = await self._behavior_based_recommendations(user_id, kb_id, db, limit // 4)
        recommendations.extend(behavior_recs)
        
        # 3. çƒ­é—¨æ¨è
        trending_recs = await self._trending_recommendations(kb_id, db, limit // 4)
        recommendations.extend(trending_recs)
        
        # 4. å»é‡å’Œæ’åº
        unique_recs = {}
        for rec in recommendations:
            if rec.document_id not in unique_recs:
                unique_recs[rec.document_id] = rec
            elif rec.score > unique_recs[rec.document_id].score:
                unique_recs[rec.document_id] = rec
                
        # æŒ‰åˆ†æ•°æ’åº
        final_recs = sorted(unique_recs.values(), key=lambda x: x.score, reverse=True)
        
        return final_recs[:limit]
        
    async def _content_based_recommendations(self, user_id: int, kb_id: int, 
                                           db: Session, limit: int) -> List[Recommendation]:
        """åŸºäºå†…å®¹çš„æ¨è"""
        recommendations = []
        
        try:
            # è·å–ç”¨æˆ·æœ€è¿‘æµè§ˆçš„æ–‡æ¡£
            recent_behaviors = await self.behavior_analyzer.get_user_behaviors(user_id, days=7)
            recent_docs = [b.document_id for b in recent_behaviors if b.action in [UserAction.VIEW, UserAction.DOWNLOAD]]
            
            if not recent_docs:
                return recommendations
                
            # è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯ï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„æ•°æ®åº“æŸ¥è¯¢ï¼‰
            # ç®€åŒ–ç¤ºä¾‹
            all_documents = []  # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–kb_idä¸‹çš„æ‰€æœ‰æ–‡æ¡£
            
            if not all_documents:
                return recommendations
                
            # åˆ†ææ–‡æ¡£å†…å®¹
            await self.content_analyzer.analyze_document_content(all_documents)
            
            # ä¸ºæ¯ä¸ªæœ€è¿‘æµè§ˆçš„æ–‡æ¡£æ‰¾ç›¸ä¼¼æ–‡æ¡£
            similar_docs = set()
            for doc_id in recent_docs[-5:]:  # åªè€ƒè™‘æœ€è¿‘5ä¸ªæ–‡æ¡£
                similar = self.content_analyzer.find_similar_documents(doc_id, 3)
                for sim_doc_id, similarity in similar:
                    if sim_doc_id not in recent_docs:  # æ’é™¤å·²æµè§ˆçš„
                        similar_docs.add((sim_doc_id, similarity))
                        
            # è½¬æ¢ä¸ºæ¨èç»“æœ
            for doc_id, similarity in list(similar_docs)[:limit]:
                rec = Recommendation(
                    document_id=doc_id,
                    score=similarity,
                    reason=f"ä¸æ‚¨æµè§ˆè¿‡çš„æ–‡æ¡£ç›¸ä¼¼ï¼ˆç›¸ä¼¼åº¦: {similarity:.2f}ï¼‰",
                    rec_type=RecommendationType.CONTENT_BASED,
                    metadata={'similarity': similarity}
                )
                recommendations.append(rec)
                
        except Exception as e:
            logger.error(f"åŸºäºå†…å®¹çš„æ¨èå¤±è´¥: {e}")
            
        return recommendations
        
    async def _behavior_based_recommendations(self, user_id: int, kb_id: int,
                                            db: Session, limit: int) -> List[Recommendation]:
        """åŸºäºè¡Œä¸ºçš„æ¨è"""
        recommendations = []
        
        try:
            # åˆ†æç”¨æˆ·å…´è¶£
            interests = await self.behavior_analyzer.analyze_user_interests(user_id, db)
            
            if not interests:
                return recommendations
                
            # åŸºäºå…´è¶£æ¨èæ–‡æ¡£ï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„æ•°æ®åº“æŸ¥è¯¢é€»è¾‘ï¼‰
            # ç®€åŒ–ç¤ºä¾‹
            for category, score in sorted(interests.items(), key=lambda x: x[1], reverse=True)[:3]:
                # è¿™é‡Œåº”è¯¥æŸ¥è¯¢è¯¥ç±»åˆ«ä¸‹çš„çƒ­é—¨æ–‡æ¡£
                # ç®€åŒ–ç¤ºä¾‹ï¼šå‡è®¾æœ‰ä¸€äº›æ–‡æ¡£ID
                doc_ids = [100 + i for i in range(limit // 3)]  # ç¤ºä¾‹æ–‡æ¡£ID
                
                for doc_id in doc_ids:
                    rec = Recommendation(
                        document_id=doc_id,
                        score=score * 0.8,  # è¡Œä¸ºæ¨èæƒé‡
                        reason=f"åŸºäºæ‚¨çš„å…´è¶£åå¥½: {category}",
                        rec_type=RecommendationType.BEHAVIOR_BASED,
                        metadata={'category': category, 'interest_score': score}
                    )
                    recommendations.append(rec)
                    
        except Exception as e:
            logger.error(f"åŸºäºè¡Œä¸ºçš„æ¨èå¤±è´¥: {e}")
            
        return recommendations
        
    async def _trending_recommendations(self, kb_id: int, db: Session, limit: int) -> List[Recommendation]:
        """çƒ­é—¨æ¨è"""
        recommendations = []
        
        try:
            # è·å–ä»Šæ—¥çƒ­é—¨æ–‡æ¡£
            today = datetime.now().strftime('%Y%m%d')
            trending_docs = await self.redis_client.zrevrange(f"trending:{today}", 0, limit - 1, withscores=True)
            
            for doc_id, score in trending_docs:
                rec = Recommendation(
                    document_id=int(doc_id),
                    score=float(score) * 0.6,  # çƒ­é—¨æ¨èæƒé‡
                    reason=f"ä»Šæ—¥çƒ­é—¨æ–‡æ¡£ï¼ˆçƒ­åº¦: {score:.1f}ï¼‰",
                    rec_type=RecommendationType.TRENDING,
                    metadata={'trending_score': float(score)}
                )
                recommendations.append(rec)
                
        except Exception as e:
            logger.error(f"çƒ­é—¨æ¨èå¤±è´¥: {e}")
            
        return recommendations
        
    async def get_user_profile(self, user_id: int, db: Session) -> UserProfile:
        """è·å–ç”¨æˆ·ç”»åƒ"""
        try:
            # åˆ†æç”¨æˆ·å…´è¶£
            interests = await self.behavior_analyzer.analyze_user_interests(user_id, db)
            
            # åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
            behaviors = await self.behavior_analyzer.get_user_behaviors(user_id, 30)
            
            # è®¡ç®—æ´»è·ƒåº¦
            activity_level = len(behaviors) / 30.0  # å¹³å‡æ¯å¤©è¡Œä¸ºæ•°
            
            # åˆ†æåå¥½æ ¼å¼
            format_counter = Counter()
            for behavior in behaviors:
                # è¿™é‡Œéœ€è¦æ ¹æ®document_idè·å–æ–‡æ¡£æ ¼å¼
                # ç®€åŒ–ç¤ºä¾‹
                format_counter['pdf'] += 1
                
            preferred_formats = [fmt for fmt, _ in format_counter.most_common(3)]
            
            # åˆ†æäº¤äº’æ¨¡å¼
            action_counter = Counter(b.action for b in behaviors)
            interaction_patterns = {
                'total_actions': len(behaviors),
                'action_distribution': dict(action_counter),
                'avg_daily_actions': activity_level,
                'most_active_hour': self._analyze_active_hours(behaviors),
                'session_duration': self._analyze_session_duration(behaviors)
            }
            
            # åˆ†æå†…å®¹ç±»åˆ«åå¥½
            categories = {}
            for behavior in behaviors:
                # è¿™é‡Œéœ€è¦æ ¹æ®document_idè·å–æ–‡æ¡£ç±»åˆ«
                # ç®€åŒ–ç¤ºä¾‹
                category = f"category_{behavior.document_id % 5}"
                categories[category] = categories.get(category, 0) + 1
                
            # å½’ä¸€åŒ–ç±»åˆ«æƒé‡
            if categories:
                max_count = max(categories.values())
                categories = {k: v / max_count for k, v in categories.items()}
                
            profile = UserProfile(
                user_id=user_id,
                interests=interests,
                categories=categories,
                activity_level=activity_level,
                preferred_formats=preferred_formats,
                interaction_patterns=interaction_patterns,
                last_updated=datetime.now()
            )
            
            # ç¼“å­˜ç”¨æˆ·ç”»åƒ
            await self.redis_client.setex(
                f"profile:{user_id}",
                24 * 3600,  # 24å°æ—¶è¿‡æœŸ
                json.dumps(profile.to_dict(), default=str)
            )
            
            return profile
            
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            return UserProfile(
                user_id=user_id,
                interests={},
                categories={},
                activity_level=0.0,
                preferred_formats=[],
                interaction_patterns={},
                last_updated=datetime.now()
            )
            
    def _analyze_active_hours(self, behaviors: List[UserBehavior]) -> int:
        """åˆ†ææœ€æ´»è·ƒæ—¶é—´"""
        if not behaviors:
            return 0
            
        hour_counter = Counter(b.timestamp.hour for b in behaviors)
        return hour_counter.most_common(1)[0][0] if hour_counter else 0
        
    def _analyze_session_duration(self, behaviors: List[UserBehavior]) -> float:
        """åˆ†æå¹³å‡ä¼šè¯æ—¶é•¿"""
        if not behaviors:
            return 0.0
            
        sessions = defaultdict(list)
        for behavior in behaviors:
            sessions[behavior.session_id].append(behavior.timestamp)
            
        durations = []
        for session_times in sessions.values():
            if len(session_times) > 1:
                session_times.sort()
                duration = (session_times[-1] - session_times[0]).total_seconds() / 60.0  # åˆ†é’Ÿ
                durations.append(duration)
                
        return sum(durations) / len(durations) if durations else 0.0

class SmartTagSuggester:
    """æ™ºèƒ½æ ‡ç­¾å»ºè®®å™¨"""
    
    def __init__(self):
        self.tag_frequency = defaultdict(int)
        self.tag_cooccurrence = defaultdict(lambda: defaultdict(int))
        
    async def analyze_document_for_tags(self, content: str, existing_tags: List[str] = None) -> List[Dict[str, Any]]:
        """åˆ†ææ–‡æ¡£å†…å®¹å»ºè®®æ ‡ç­¾"""
        suggestions = []
        
        try:
            # 1. å…³é”®è¯æå–
            keywords = jieba.analyse.extract_tags(content, topK=20, withWeight=True)
            
            for keyword, weight in keywords:
                if len(keyword) > 1:  # è¿‡æ»¤å•å­—è¯
                    suggestions.append({
                        'tag': keyword,
                        'confidence': weight,
                        'reason': 'å…³é”®è¯æå–',
                        'type': 'keyword'
                    })
                    
            # 2. åŸºäºç°æœ‰æ ‡ç­¾çš„ç›¸å…³æ ‡ç­¾å»ºè®®
            if existing_tags:
                related_tags = await self._suggest_related_tags(existing_tags)
                for tag, score in related_tags:
                    suggestions.append({
                        'tag': tag,
                        'confidence': score,
                        'reason': f'ä¸æ ‡ç­¾ "{", ".join(existing_tags)}" ç›¸å…³',
                        'type': 'related'
                    })
                    
            # 3. å®ä½“è¯†åˆ«ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            entities = self._extract_entities(content)
            for entity, entity_type in entities:
                suggestions.append({
                    'tag': entity,
                    'confidence': 0.7,
                    'reason': f'{entity_type}å®ä½“',
                    'type': 'entity'
                })
                
            # 4. å»é‡å’Œæ’åº
            unique_suggestions = {}
            for suggestion in suggestions:
                tag = suggestion['tag']
                if tag not in unique_suggestions or suggestion['confidence'] > unique_suggestions[tag]['confidence']:
                    unique_suggestions[tag] = suggestion
                    
            # æŒ‰ç½®ä¿¡åº¦æ’åº
            final_suggestions = sorted(unique_suggestions.values(), 
                                     key=lambda x: x['confidence'], reverse=True)
            
            return final_suggestions[:10]  # è¿”å›å‰10ä¸ªå»ºè®®
            
        except Exception as e:
            logger.error(f"æ ‡ç­¾å»ºè®®å¤±è´¥: {e}")
            return []
            
    def _extract_entities(self, text: str) -> List[Tuple[str, str]]:
        """æå–å®ä½“ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        entities = []
        
        # ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
        patterns = {
            'æ—¶é—´': r'\d{4}å¹´|\d{1,2}æœˆ|\d{1,2}æ—¥',
            'æ•°å­—': r'\d+(?:\.\d+)?%|\d+(?:\.\d+)?ä¸‡|\d+(?:\.\d+)?äº¿',
            'ç»„ç»‡': r'å…¬å¸|ä¼ä¸š|æœºæ„|éƒ¨é—¨|å­¦æ ¡|å¤§å­¦',
            'åœ°ç‚¹': r'åŒ—äº¬|ä¸Šæµ·|å¹¿å·|æ·±åœ³|æ­å·|å—äº¬|æ­¦æ±‰|æˆéƒ½|é‡åº†|è¥¿å®‰'
        }
        
        for entity_type, pattern in patterns.items():
            matches = re.findall(pattern, text)
            for match in matches:
                entities.append((match, entity_type))
                
        return entities
        
    async def _suggest_related_tags(self, existing_tags: List[str]) -> List[Tuple[str, float]]:
        """åŸºäºå…±ç°å…³ç³»å»ºè®®ç›¸å…³æ ‡ç­¾"""
        related_tags = defaultdict(float)
        
        for tag in existing_tags:
            # è¿™é‡Œåº”è¯¥ä»å†å²æ•°æ®ä¸­è·å–æ ‡ç­¾å…±ç°å…³ç³»
            # ç®€åŒ–ç¤ºä¾‹
            if tag in ['æŠ€æœ¯', 'å¼€å‘']:
                related_tags['ç¼–ç¨‹'] += 0.8
                related_tags['è½¯ä»¶'] += 0.7
            elif tag in ['ç®¡ç†', 'é¡¹ç›®']:
                related_tags['å›¢é˜Ÿ'] += 0.8
                related_tags['è®¡åˆ’'] += 0.6
                
        return list(related_tags.items())
        
    async def update_tag_statistics(self, document_tags: List[str]):
        """æ›´æ–°æ ‡ç­¾ç»Ÿè®¡ä¿¡æ¯"""
        # æ›´æ–°æ ‡ç­¾é¢‘ç‡
        for tag in document_tags:
            self.tag_frequency[tag] += 1
            
        # æ›´æ–°æ ‡ç­¾å…±ç°å…³ç³»
        for i, tag1 in enumerate(document_tags):
            for tag2 in document_tags[i+1:]:
                self.tag_cooccurrence[tag1][tag2] += 1
                self.tag_cooccurrence[tag2][tag1] += 1

class PersonalizedSearch:
    """ä¸ªæ€§åŒ–æœç´¢"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        
    async def personalized_search(self, user_id: int, query: str, kb_id: int,
                                db: Session, limit: int = 20) -> List[Dict[str, Any]]:
        """ä¸ªæ€§åŒ–æœç´¢"""
        try:
            # 1. è·å–ç”¨æˆ·ç”»åƒ
            profile_data = await self.redis_client.get(f"profile:{user_id}")
            if profile_data:
                profile = json.loads(profile_data)
                user_interests = profile.get('interests', {})
            else:
                user_interests = {}
                
            # 2. æŸ¥è¯¢æ‰©å±•
            expanded_query = await self._expand_query(query, user_interests)
            
            # 3. æ‰§è¡ŒåŸºç¡€æœç´¢ï¼ˆè¿™é‡Œéœ€è¦è°ƒç”¨å®é™…çš„æœç´¢å‡½æ•°ï¼‰
            # ç®€åŒ–ç¤ºä¾‹
            base_results = []  # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æœç´¢ç»“æœ
            
            # 4. ä¸ªæ€§åŒ–é‡æ’åº
            personalized_results = await self._rerank_results(
                base_results, user_interests, user_id
            )
            
            # 5. è®°å½•æœç´¢è¡Œä¸º
            await self._record_search_behavior(user_id, query, kb_id)
            
            return personalized_results[:limit]
            
        except Exception as e:
            logger.error(f"ä¸ªæ€§åŒ–æœç´¢å¤±è´¥: {e}")
            return []
            
    async def _expand_query(self, query: str, user_interests: Dict[str, float]) -> str:
        """æŸ¥è¯¢æ‰©å±•"""
        expanded_terms = [query]
        
        # åŸºäºç”¨æˆ·å…´è¶£æ·»åŠ ç›¸å…³è¯æ±‡
        query_words = set(jieba.cut(query))
        
        for interest, weight in user_interests.items():
            if weight > 0.5:  # åªè€ƒè™‘é«˜æƒé‡å…´è¶£
                # è¿™é‡Œå¯ä»¥æ·»åŠ åŒä¹‰è¯è¯å…¸æˆ–è¯å‘é‡æ¨¡å‹æ¥æ‰©å±•æŸ¥è¯¢
                expanded_terms.append(interest)
                
        return ' '.join(expanded_terms)
        
    async def _rerank_results(self, results: List[Dict[str, Any]], 
                            user_interests: Dict[str, float], user_id: int) -> List[Dict[str, Any]]:
        """ä¸ªæ€§åŒ–é‡æ’åº"""
        if not results or not user_interests:
            return results
            
        # ä¸ºæ¯ä¸ªç»“æœè®¡ç®—ä¸ªæ€§åŒ–åˆ†æ•°
        for result in results:
            base_score = result.get('score', 0.0)
            
            # åŸºäºç”¨æˆ·å…´è¶£è°ƒæ•´åˆ†æ•°
            interest_boost = 0.0
            doc_categories = result.get('categories', [])
            
            for category in doc_categories:
                if category in user_interests:
                    interest_boost += user_interests[category] * 0.3
                    
            # åŸºäºç”¨æˆ·å†å²è¡Œä¸ºè°ƒæ•´åˆ†æ•°
            behavior_boost = await self._calculate_behavior_boost(user_id, result['id'])
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            result['personalized_score'] = base_score + interest_boost + behavior_boost
            
        # æŒ‰ä¸ªæ€§åŒ–åˆ†æ•°é‡æ–°æ’åº
        return sorted(results, key=lambda x: x.get('personalized_score', 0), reverse=True)
        
    async def _calculate_behavior_boost(self, user_id: int, doc_id: int) -> float:
        """è®¡ç®—åŸºäºå†å²è¡Œä¸ºçš„åˆ†æ•°æå‡"""
        boost = 0.0
        
        try:
            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æµè§ˆè¿‡ç›¸ä¼¼æ–‡æ¡£
            behavior_key = f"user_docs:{user_id}"
            viewed_docs = await self.redis_client.smembers(behavior_key)
            
            if str(doc_id) in viewed_docs:
                boost -= 0.2  # å·²æµè§ˆè¿‡çš„æ–‡æ¡£é™æƒ
            else:
                # æ£€æŸ¥æ˜¯å¦æµè§ˆè¿‡ç›¸ä¼¼ç±»å‹çš„æ–‡æ¡£
                # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„ç›¸ä¼¼åº¦è®¡ç®—
                boost += 0.1  # æ–°æ–‡æ¡£è½»å¾®åŠ æƒ
                
        except Exception as e:
            logger.error(f"è®¡ç®—è¡Œä¸ºåŠ æƒå¤±è´¥: {e}")
            
        return boost
        
    async def _record_search_behavior(self, user_id: int, query: str, kb_id: int):
        """è®°å½•æœç´¢è¡Œä¸º"""
        search_log = {
            'user_id': user_id,
            'query': query,
            'kb_id': kb_id,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æœç´¢å†å²
        await self.redis_client.lpush(
            f"search_history:{user_id}",
            json.dumps(search_log)
        )
        
        # é™åˆ¶æœç´¢å†å²é•¿åº¦
        await self.redis_client.ltrim(f"search_history:{user_id}", 0, 99)

# å…¨å±€æ¨èå¼•æ“å®ä¾‹
recommendation_engine = None
tag_suggester = None
personalized_search = None

def init_recommendation_system(redis_client) -> Tuple[RecommendationEngine, SmartTagSuggester, PersonalizedSearch]:
    """åˆå§‹åŒ–æ¨èç³»ç»Ÿ"""
    global recommendation_engine, tag_suggester, personalized_search
    
    recommendation_engine = RecommendationEngine(redis_client)
    tag_suggester = SmartTagSuggester()
    personalized_search = PersonalizedSearch(redis_client)
    
    logger.info("ğŸ¯ Recommendation - æ™ºèƒ½æ¨èç³»ç»Ÿå·²åˆå§‹åŒ–")
    return recommendation_engine, tag_suggester, personalized_search

def get_recommendation_engine() -> Optional[RecommendationEngine]:
    """è·å–æ¨èå¼•æ“å®ä¾‹"""
    return recommendation_engine

def get_tag_suggester() -> Optional[SmartTagSuggester]:
    """è·å–æ ‡ç­¾å»ºè®®å™¨å®ä¾‹"""
    return tag_suggester

def get_personalized_search() -> Optional[PersonalizedSearch]:
    """è·å–ä¸ªæ€§åŒ–æœç´¢å®ä¾‹"""
    return personalized_search
