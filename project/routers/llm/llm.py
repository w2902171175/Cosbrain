# project/routers/llm.py
"""
LLMæ¨¡å—è·¯ç”±å±‚ - ä¼˜åŒ–ç‰ˆæœ¬
é›†æˆä¼˜åŒ–æ¡†æ¶æä¾›é«˜æ€§èƒ½çš„LLMé…ç½®ç®¡ç†ã€å¯¹è¯ç®¡ç†å’Œæ¨ç†API
æ”¯æŒåˆ†å¸ƒå¼ç¼“å­˜ã€æµå¼å“åº”ã€è´Ÿè½½å‡è¡¡ç­‰é«˜çº§åŠŸèƒ½
"""
from typing import Optional, List, Dict, Any, AsyncGenerator
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query, Body
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
import json
import asyncio
import logging
from datetime import datetime, timedelta

# æ ¸å¿ƒå¯¼å…¥
from project.database import get_db
from project.utils.core.error_decorators import handle_database_errors, database_transaction
from project.utils.optimization.router_optimization import optimized_route
import project.schemas as schemas
from project.services.llm_service import (
    LLMProviderService, LLMConfigService, LLMConversationService, 
    LLMInferenceService, LLMMonitoringService, LLMUtilities
)

# å·¥å…·å¯¼å…¥
from project.utils.optimization.production_utils import cache_manager
from project.utils import get_current_user_id

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/llm", tags=["LLMå¤§è¯­è¨€æ¨¡å‹"])

# ===== LLMæä¾›å•†ç®¡ç†è·¯ç”± =====

@router.get("/providers", response_model=schemas.PaginatedResponse)
@optimized_route
@handle_database_errors
async def get_llm_providers(
    skip: int = Query(0, ge=0, description="è·³è¿‡çš„è®°å½•æ•°"),
    limit: int = Query(50, ge=1, le=100, description="è¿”å›çš„è®°å½•æ•°"),
    provider_type: Optional[str] = Query(None, description="æä¾›å•†ç±»å‹è¿‡æ»¤"),
    is_active: Optional[bool] = Query(None, description="æ´»è·ƒçŠ¶æ€è¿‡æ»¤"),
    db: Session = Depends(get_db)
):
    """
    è·å–LLMæä¾›å•†åˆ—è¡¨
    
    - **skip**: åˆ†é¡µè·³è¿‡çš„è®°å½•æ•°
    - **limit**: è¿”å›çš„è®°å½•æ•°é‡é™åˆ¶  
    - **provider_type**: æŒ‰æä¾›å•†ç±»å‹è¿‡æ»¤
    - **is_active**: æŒ‰æ´»è·ƒçŠ¶æ€è¿‡æ»¤
    """
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"llm_providers_{skip}_{limit}_{provider_type}_{is_active}"
        cached_result = cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        providers, total = LLMProviderService.get_llm_providers_optimized(
            db, skip, limit, provider_type, is_active
        )
        
        # æ„å»ºå“åº”
        provider_list = [{
            "id": p.id,
            "name": p.name,
            "provider_type": p.provider_type,
            "base_url": p.base_url,
            "is_active": p.is_active,
            "supported_models": getattr(p, 'supported_models', []),
            "created_at": p.created_at
        } for p in providers]
        
        result = {
            "items": provider_list,
            "total": total,
            "skip": skip,
            "limit": limit,
            "has_more": skip + limit < total
        }
        
        # ç¼“å­˜ç»“æœ
        cache_manager.set(cache_key, result, ttl=600)  # 10åˆ†é’Ÿç¼“å­˜
        
        logger.info(f"è·å–LLMæä¾›å•†åˆ—è¡¨: {len(provider_list)} ä¸ªæä¾›å•†")
        return result
        
    except Exception as e:
        logger.error(f"è·å–LLMæä¾›å•†åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–LLMæä¾›å•†åˆ—è¡¨å¤±è´¥")

@router.post("/providers", response_model=schemas.Response)
@optimized_route
@handle_database_errors
@database_transaction
async def create_llm_provider(
    provider_data: Dict[str, Any] = Body(..., description="LLMæä¾›å•†æ•°æ®"),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    åˆ›å»ºæ–°çš„LLMæä¾›å•†
    
    - **provider_data**: LLMæä¾›å•†æ•°æ®ï¼ŒåŒ…å«åç§°ã€ç±»å‹ã€APIåœ°å€ç­‰
    """
    try:
        # åˆ›å»ºæä¾›å•†
        new_provider = LLMProviderService.create_llm_provider_optimized(
            db, provider_data
        )
        
        # åå°ä»»åŠ¡ï¼šæ¸…ç†ç›¸å…³ç¼“å­˜
        background_tasks.add_task(
            cache_manager.delete_pattern, 
            "llm_providers_*"
        )
        
        result = {
            "id": new_provider.id,
            "name": new_provider.name,
            "provider_type": new_provider.provider_type,
            "base_url": new_provider.base_url,
            "is_active": new_provider.is_active,
            "created_at": new_provider.created_at
        }
        
        logger.info(f"ç”¨æˆ· {current_user_id} åˆ›å»ºLLMæä¾›å•†: {new_provider.name}")
        return {
            "message": "LLMæä¾›å•†åˆ›å»ºæˆåŠŸ",
            "data": result
        }
        
    except ValueError as e:
        logger.warning(f"LLMæä¾›å•†åˆ›å»ºå‚æ•°é”™è¯¯: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"åˆ›å»ºLLMæä¾›å•†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="åˆ›å»ºLLMæä¾›å•†å¤±è´¥")

# ===== ç”¨æˆ·LLMé…ç½®ç®¡ç†è·¯ç”± =====

@router.get("/configs", response_model=schemas.PaginatedResponse)
@optimized_route
@handle_database_errors
async def get_user_llm_configs(
    skip: int = Query(0, ge=0, description="è·³è¿‡çš„è®°å½•æ•°"),
    limit: int = Query(50, ge=1, le=100, description="è¿”å›çš„è®°å½•æ•°"),
    provider_type: Optional[str] = Query(None, description="æä¾›å•†ç±»å‹è¿‡æ»¤"),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    è·å–ç”¨æˆ·LLMé…ç½®åˆ—è¡¨
    
    - **skip**: åˆ†é¡µè·³è¿‡çš„è®°å½•æ•°
    - **limit**: è¿”å›çš„è®°å½•æ•°é‡é™åˆ¶
    - **provider_type**: æŒ‰æä¾›å•†ç±»å‹è¿‡æ»¤
    """
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"user_llm_configs_{current_user_id}_{skip}_{limit}_{provider_type}"
        cached_result = cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        configs, total = LLMConfigService.get_user_llm_configs_optimized(
            db, current_user_id, skip, limit, provider_type
        )
        
        # æ„å»ºå“åº”
        config_list = [LLMUtilities.build_safe_response_dict(config) for config in configs]
        result = {
            "items": config_list,
            "total": total,
            "skip": skip,
            "limit": limit,
            "has_more": skip + limit < total
        }
        
        # ç¼“å­˜ç»“æœ
        cache_manager.set(cache_key, result, ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
        
        logger.info(f"ç”¨æˆ· {current_user_id} è·å–LLMé…ç½®åˆ—è¡¨: {len(config_list)} ä¸ªé…ç½®")
        return result
        
    except Exception as e:
        logger.error(f"è·å–LLMé…ç½®åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–LLMé…ç½®åˆ—è¡¨å¤±è´¥")

@router.post("/configs", response_model=schemas.Response)
@optimized_route
@handle_database_errors
@database_transaction
async def create_llm_config(
    config_data: Dict[str, Any] = Body(..., description="LLMé…ç½®æ•°æ®"),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    åˆ›å»ºæ–°çš„LLMé…ç½®
    
    - **config_data**: LLMé…ç½®æ•°æ®ï¼ŒåŒ…å«æä¾›å•†IDã€é…ç½®åç§°ã€æ¨¡å‹å‚æ•°ç­‰
    """
    try:
        # åˆ›å»ºé…ç½®
        new_config = LLMConfigService.create_llm_config_optimized(
            db, current_user_id, config_data
        )
        
        # åå°ä»»åŠ¡ï¼šæ¸…ç†ç›¸å…³ç¼“å­˜
        background_tasks.add_task(
            LLMUtilities.clear_user_cache, 
            current_user_id
        )
        
        result = LLMUtilities.build_safe_response_dict(new_config)
        
        logger.info(f"ç”¨æˆ· {current_user_id} åˆ›å»ºLLMé…ç½® {new_config.id}")
        return {
            "message": "LLMé…ç½®åˆ›å»ºæˆåŠŸ",
            "data": result
        }
        
    except ValueError as e:
        logger.warning(f"LLMé…ç½®åˆ›å»ºå‚æ•°é”™è¯¯: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"åˆ›å»ºLLMé…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="åˆ›å»ºLLMé…ç½®å¤±è´¥")

# ===== å¯¹è¯ç®¡ç†è·¯ç”± =====

@router.get("/conversations", response_model=schemas.PaginatedResponse)
@optimized_route
@handle_database_errors
async def get_user_conversations(
    skip: int = Query(0, ge=0, description="è·³è¿‡çš„è®°å½•æ•°"),
    limit: int = Query(50, ge=1, le=100, description="è¿”å›çš„è®°å½•æ•°"),
    with_messages: bool = Query(False, description="æ˜¯å¦åŒ…å«æ¶ˆæ¯å†…å®¹"),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨
    
    - **skip**: åˆ†é¡µè·³è¿‡çš„è®°å½•æ•°
    - **limit**: è¿”å›çš„è®°å½•æ•°é‡é™åˆ¶
    - **with_messages**: æ˜¯å¦åŒ…å«æ¶ˆæ¯å†…å®¹
    """
    try:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"user_conversations_{current_user_id}_{skip}_{limit}_{with_messages}"
        cached_result = cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        conversations, total = LLMConversationService.get_user_conversations_optimized(
            db, current_user_id, skip, limit, with_messages
        )
        
        # æ„å»ºå“åº”
        conversation_list = []
        for conv in conversations:
            conv_dict = {
                "id": conv.id,
                "title": conv.title,
                "model_name": conv.model_name,
                "message_count": len(conv.messages) if hasattr(conv, 'messages') else 0,
                "created_at": conv.created_at,
                "updated_at": conv.updated_at
            }
            
            if with_messages and hasattr(conv, 'messages'):
                conv_dict["messages"] = [{
                    "id": msg.id,
                    "role": msg.role,
                    "content": msg.content,
                    "created_at": msg.created_at
                } for msg in conv.messages]
            
            conversation_list.append(conv_dict)
        
        result = {
            "items": conversation_list,
            "total": total,
            "skip": skip,
            "limit": limit,
            "has_more": skip + limit < total
        }
        
        # ç¼“å­˜ç»“æœ
        cache_manager.set(cache_key, result, ttl=180)  # 3åˆ†é’Ÿç¼“å­˜
        
        logger.info(f"ç”¨æˆ· {current_user_id} è·å–å¯¹è¯åˆ—è¡¨: {len(conversation_list)} ä¸ªå¯¹è¯")
        return result
        
    except Exception as e:
        logger.error(f"è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥")

@router.post("/conversations", response_model=schemas.Response)
@optimized_route
@handle_database_errors
@database_transaction
async def create_conversation(
    conversation_data: Dict[str, Any] = Body(..., description="å¯¹è¯æ•°æ®"),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    åˆ›å»ºæ–°çš„å¯¹è¯
    
    - **conversation_data**: å¯¹è¯æ•°æ®ï¼ŒåŒ…å«æ ‡é¢˜ã€æ¨¡å‹åç§°ã€ç³»ç»Ÿæç¤ºç­‰
    """
    try:
        # åˆ›å»ºå¯¹è¯
        new_conversation = LLMConversationService.create_conversation_optimized(
            db, current_user_id, conversation_data
        )
        
        # åå°ä»»åŠ¡ï¼šæ¸…ç†ç›¸å…³ç¼“å­˜
        background_tasks.add_task(
            cache_manager.delete_pattern, 
            f"user_conversations_{current_user_id}_*"
        )
        
        result = {
            "id": new_conversation.id,
            "title": new_conversation.title,
            "model_name": new_conversation.model_name,
            "system_prompt": new_conversation.system_prompt,
            "created_at": new_conversation.created_at
        }
        
        logger.info(f"ç”¨æˆ· {current_user_id} åˆ›å»ºå¯¹è¯ {new_conversation.id}")
        return {
            "message": "å¯¹è¯åˆ›å»ºæˆåŠŸ",
            "data": result
        }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå¯¹è¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="åˆ›å»ºå¯¹è¯å¤±è´¥")

# ===== LLMæ¨ç†è·¯ç”± =====

@router.post("/chat", response_model=schemas.Response)
@optimized_route
@handle_database_errors
async def chat_with_llm(
    chat_request: Dict[str, Any] = Body(
        ..., 
        description="èŠå¤©è¯·æ±‚",
        example={
            "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            "conversation_id": None,
            "model": "gpt-3.5-turbo",
            "temperature": 0.7,
            "max_tokens": 2048,
            "system_prompt": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"
        }
    ),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    ä¸LLMè¿›è¡Œå¯¹è¯
    
    - **content**: æ¶ˆæ¯å†…å®¹
    - **conversation_id**: å¯¹è¯IDï¼ˆå¯é€‰ï¼Œæ–°å¯¹è¯åˆ™ä¸ä¼ ï¼‰
    - **model**: ä½¿ç”¨çš„æ¨¡å‹åç§°
    - **temperature**: æ¸©åº¦å‚æ•°
    - **max_tokens**: æœ€å¤§ç”Ÿæˆé•¿åº¦
    - **system_prompt**: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
    """
    try:
        # éªŒè¯è¯·æ±‚å‚æ•°
        content = chat_request.get("content")
        if not content or not content.strip():
            raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        conversation_id = chat_request.get("conversation_id")
        
        # æ‰§è¡ŒLLMæ¨ç†
        response = await LLMInferenceService.generate_response_optimized(
            db, current_user_id, conversation_id, chat_request, stream=False
        )
        
        if response.get("status") == "error":
            raise HTTPException(
                status_code=400, 
                detail=response.get("message", "LLMæ¨ç†å¤±è´¥")
            )
        
        # åå°ä»»åŠ¡ï¼šæ¸…ç†ç›¸å…³ç¼“å­˜
        background_tasks.add_task(
            cache_manager.delete_pattern, 
            f"user_conversations_{current_user_id}_*"
        )
        
        logger.info(f"ç”¨æˆ· {current_user_id} å®ŒæˆLLMå¯¹è¯ï¼Œå¯¹è¯ID: {response.get('conversation_id')}")
        return {
            "message": "å¯¹è¯æˆåŠŸ",
            "data": response
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"LLMå¯¹è¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="LLMå¯¹è¯æœåŠ¡é”™è¯¯")

@router.post("/chat/stream")
@optimized_route
@handle_database_errors
async def chat_with_llm_stream(
    chat_request: Dict[str, Any] = Body(..., description="æµå¼èŠå¤©è¯·æ±‚"),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    ä¸LLMè¿›è¡Œæµå¼å¯¹è¯
    
    - **content**: æ¶ˆæ¯å†…å®¹
    - **conversation_id**: å¯¹è¯IDï¼ˆå¯é€‰ï¼‰
    - **model**: ä½¿ç”¨çš„æ¨¡å‹åç§°
    - **temperature**: æ¸©åº¦å‚æ•°
    - **max_tokens**: æœ€å¤§ç”Ÿæˆé•¿åº¦
    """
    try:
        # éªŒè¯è¯·æ±‚å‚æ•°
        content = chat_request.get("content")
        if not content or not content.strip():
            raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        conversation_id = chat_request.get("conversation_id")
        
        async def generate_stream():
            try:
                # æ‰§è¡Œæµå¼æ¨ç†
                response = await LLMInferenceService.generate_response_optimized(
                    db, current_user_id, conversation_id, chat_request, stream=True
                )
                
                if response.get("status") == "error":
                    yield f"data: {json.dumps({'error': response.get('message')})}\n\n"
                    return
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                full_response = response.get("response", "")
                words = full_response.split()
                
                for i, word in enumerate(words):
                    chunk_data = {
                        "content": word + (" " if i < len(words) - 1 else ""),
                        "conversation_id": response.get("conversation_id"),
                        "finished": i == len(words) - 1
                    }
                    yield f"data: {json.dumps(chunk_data)}\n\n"
                    
                    # æ¨¡æ‹Ÿå»¶è¿Ÿ
                    await asyncio.sleep(0.05)
                
                # å‘é€ç»“æŸæ ‡å¿—
                yield f"data: {json.dumps({'finished': True, 'conversation_id': response.get('conversation_id')})}\n\n"
                
            except Exception as e:
                logger.error(f"æµå¼å¯¹è¯ç”Ÿæˆå¤±è´¥: {e}")
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
        
        return StreamingResponse(
            generate_stream(),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Content-Type": "text/event-stream"
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æµå¼LLMå¯¹è¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="æµå¼LLMå¯¹è¯æœåŠ¡é”™è¯¯")

# ===== ç›‘æ§å’Œç»Ÿè®¡è·¯ç”± =====

@router.get("/statistics", response_model=schemas.Response)
@optimized_route
@handle_database_errors
async def get_llm_statistics(
    start_date: Optional[str] = Query(None, description="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)"),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """
    è·å–LLMä½¿ç”¨ç»Ÿè®¡
    
    - **start_date**: ç»Ÿè®¡å¼€å§‹æ—¥æœŸ
    - **end_date**: ç»Ÿè®¡ç»“æŸæ—¥æœŸ
    """
    try:
        # è§£ææ—¥æœŸå‚æ•°
        start_dt = None
        end_dt = None
        
        if start_date:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
        if end_date:
            end_dt = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"llm_stats_{current_user_id}_{start_date}_{end_date}"
        cached_result = cache_manager.get(cache_key)
        if cached_result:
            return cached_result
        
        # è·å–ç»Ÿè®¡æ•°æ®
        stats = await LLMMonitoringService.get_usage_statistics_optimized(
            db, current_user_id, start_dt, end_dt
        )
        
        result = {
            "message": "è·å–LLMç»Ÿè®¡æˆåŠŸ",
            "data": stats
        }
        
        # ç¼“å­˜ç»“æœ
        cache_manager.set(cache_key, result, ttl=1800)  # 30åˆ†é’Ÿç¼“å­˜
        
        return result
        
    except ValueError as e:
        logger.warning(f"æ—¥æœŸå‚æ•°é”™è¯¯: {e}")
        raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
    except Exception as e:
        logger.error(f"è·å–LLMç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–LLMç»Ÿè®¡å¤±è´¥")

@router.get("/health", response_model=schemas.Response)
@optimized_route
async def llm_health_check():
    """LLMæ¨¡å—å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥ç¼“å­˜è¿æ¥
        cache_status = "healthy" if cache_manager.is_connected() else "error"
        
        health_data = {
            "status": "healthy",
            "module": "LLM",
            "timestamp": datetime.now().isoformat(),
            "cache_status": cache_status,
            "distributed_cache": "enabled",
            "features": [
                "å¯¹è¯ç®¡ç†",
                "æµå¼å“åº”", 
                "å¤šæä¾›å•†æ”¯æŒ",
                "åˆ†å¸ƒå¼ç¼“å­˜",
                "ä½¿ç”¨ç»Ÿè®¡"
            ],
            "version": "2.0.0"
        }
        
        logger.info("LLMæ¨¡å—å¥åº·æ£€æŸ¥")
        return {
            "message": "LLMæ¨¡å—è¿è¡Œæ­£å¸¸",
            "data": health_data
        }
        
    except Exception as e:
        logger.error(f"LLMå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return {
            "message": "LLMæ¨¡å—å¥åº·æ£€æŸ¥å¼‚å¸¸",
            "data": {
                "status": "error",
                "error": str(e)
            }
        }

# æ¨¡å—åŠ è½½æ—¥å¿—
logger.info("ğŸ¤– LLM Module - å¤§è¯­è¨€æ¨¡å‹æ¨¡å—å·²åŠ è½½")
