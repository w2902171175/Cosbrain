# project/routers/llm/baseline_comparator.py
"""
LLMæ¨¡å—æ€§èƒ½åŸºçº¿å¯¹æ¯”ç³»ç»Ÿ
æä¾›è¯¦ç»†çš„æ€§èƒ½åŸºçº¿ç®¡ç†å’Œå¯¹æ¯”åˆ†æ
"""
import json
import os
import time
import statistics
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from collections import defaultdict, deque
import logging
import threading
from pathlib import Path

from .cache_service import get_llm_cache_service
from .distributed_cache import get_llm_cache

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®"""
    name: str
    value: float
    timestamp: datetime
    unit: str = ""
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'name': self.name,
            'value': self.value,
            'timestamp': self.timestamp.isoformat(),
            'unit': self.unit,
            'description': self.description
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PerformanceMetric':
        return cls(
            name=data['name'],
            value=data['value'],
            timestamp=datetime.fromisoformat(data['timestamp']),
            unit=data.get('unit', ''),
            description=data.get('description', '')
        )

@dataclass
class BaselineSnapshot:
    """æ€§èƒ½åŸºçº¿å¿«ç…§"""
    name: str
    created_at: datetime
    metrics: Dict[str, PerformanceMetric]
    metadata: Dict[str, Any] = field(default_factory=dict)
    description: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'name': self.name,
            'created_at': self.created_at.isoformat(),
            'metrics': {name: metric.to_dict() for name, metric in self.metrics.items()},
            'metadata': self.metadata,
            'description': self.description
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'BaselineSnapshot':
        return cls(
            name=data['name'],
            created_at=datetime.fromisoformat(data['created_at']),
            metrics={
                name: PerformanceMetric.from_dict(metric_data)
                for name, metric_data in data['metrics'].items()
            },
            metadata=data.get('metadata', {}),
            description=data.get('description', '')
        )

@dataclass
class ComparisonResult:
    """å¯¹æ¯”ç»“æœ"""
    metric_name: str
    baseline_value: float
    current_value: float
    deviation_percent: float
    deviation_absolute: float
    status: str  # 'improved', 'degraded', 'stable'
    significance: str  # 'low', 'medium', 'high', 'critical'
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'metric_name': self.metric_name,
            'baseline_value': self.baseline_value,
            'current_value': self.current_value,
            'deviation_percent': self.deviation_percent,
            'deviation_absolute': self.deviation_absolute,
            'status': self.status,
            'significance': self.significance
        }

class LLMBaselineComparator:
    """LLMæ€§èƒ½åŸºçº¿å¯¹æ¯”å™¨"""
    
    def __init__(self, data_dir: str = "performance_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.cache_service = get_llm_cache_service()
        self.cache = get_llm_cache()
        
        # åŸºçº¿å­˜å‚¨
        self.baselines: Dict[str, BaselineSnapshot] = {}
        self.current_baseline: Optional[BaselineSnapshot] = None
        
        # æ€§èƒ½å†å²æ•°æ®
        self.performance_history: deque = deque(maxlen=10000)
        self.metric_trends: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        
        # é…ç½®
        self.metric_definitions = self._init_metric_definitions()
        self.significance_thresholds = self._init_significance_thresholds()
        
        # åŠ è½½æ•°æ®
        self._load_baselines()
        self._load_performance_history()
        
        logger.info("ğŸ“Š LLM Baseline - æ€§èƒ½åŸºçº¿å¯¹æ¯”å™¨å·²åˆå§‹åŒ–")
    
    def _init_metric_definitions(self) -> Dict[str, Dict[str, Any]]:
        """åˆå§‹åŒ–æŒ‡æ ‡å®šä¹‰"""
        return {
            'cache_hit_rate': {
                'unit': '%',
                'description': 'ç¼“å­˜å‘½ä¸­ç‡',
                'higher_is_better': True,
                'critical_threshold': 20,  # åå·®è¶…è¿‡20%ä¸ºå…³é”®
                'warning_threshold': 10    # åå·®è¶…è¿‡10%ä¸ºè­¦å‘Š
            },
            'cache_response_time': {
                'unit': 'ms',
                'description': 'ç¼“å­˜å“åº”æ—¶é—´',
                'higher_is_better': False,
                'critical_threshold': 50,
                'warning_threshold': 25
            },
            'api_response_time': {
                'unit': 'ms',
                'description': 'APIå“åº”æ—¶é—´',
                'higher_is_better': False,
                'critical_threshold': 100,
                'warning_threshold': 50
            },
            'system_health_score': {
                'unit': 'åˆ†',
                'description': 'ç³»ç»Ÿå¥åº·è¯„åˆ†',
                'higher_is_better': True,
                'critical_threshold': 20,
                'warning_threshold': 10
            },
            'error_rate': {
                'unit': '%',
                'description': 'é”™è¯¯ç‡',
                'higher_is_better': False,
                'critical_threshold': 100,  # é”™è¯¯ç‡å¢åŠ 100%ä¸ºå…³é”®
                'warning_threshold': 50     # é”™è¯¯ç‡å¢åŠ 50%ä¸ºè­¦å‘Š
            },
            'memory_usage': {
                'unit': 'MB',
                'description': 'å†…å­˜ä½¿ç”¨é‡',
                'higher_is_better': False,
                'critical_threshold': 50,
                'warning_threshold': 25
            },
            'concurrent_requests': {
                'unit': 'req/s',
                'description': 'å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›',
                'higher_is_better': True,
                'critical_threshold': 30,
                'warning_threshold': 15
            },
            'redis_availability': {
                'unit': '%',
                'description': 'Rediså¯ç”¨æ€§',
                'higher_is_better': True,
                'critical_threshold': 10,
                'warning_threshold': 5
            }
        }
    
    def _init_significance_thresholds(self) -> Dict[str, Dict[str, float]]:
        """åˆå§‹åŒ–æ˜¾è‘—æ€§é˜ˆå€¼"""
        return {
            'low': {'min': 0, 'max': 5},      # 0-5%åå·®
            'medium': {'min': 5, 'max': 15},  # 5-15%åå·®
            'high': {'min': 15, 'max': 30},   # 15-30%åå·®
            'critical': {'min': 30, 'max': float('inf')}  # >30%åå·®
        }
    
    def _load_baselines(self):
        """åŠ è½½åŸºçº¿æ•°æ®"""
        try:
            baseline_file = self.data_dir / "baselines.json"
            if baseline_file.exists():
                with open(baseline_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                for name, baseline_data in data.items():
                    self.baselines[name] = BaselineSnapshot.from_dict(baseline_data)
                
                # è®¾ç½®å½“å‰åŸºçº¿ä¸ºæœ€æ–°çš„
                if self.baselines:
                    latest_baseline = max(
                        self.baselines.values(),
                        key=lambda b: b.created_at
                    )
                    self.current_baseline = latest_baseline
                
                logger.info(f"åŠ è½½äº† {len(self.baselines)} ä¸ªåŸºçº¿")
            
        except Exception as e:
            logger.error(f"åŠ è½½åŸºçº¿æ•°æ®å¤±è´¥: {e}")
    
    def _save_baselines(self):
        """ä¿å­˜åŸºçº¿æ•°æ®"""
        try:
            baseline_file = self.data_dir / "baselines.json"
            data = {name: baseline.to_dict() for name, baseline in self.baselines.items()}
            
            with open(baseline_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info("åŸºçº¿æ•°æ®å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºçº¿æ•°æ®å¤±è´¥: {e}")
    
    def _load_performance_history(self):
        """åŠ è½½æ€§èƒ½å†å²æ•°æ®"""
        try:
            history_file = self.data_dir / "performance_history.json"
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                for record in data.get('history', []):
                    timestamp = datetime.fromisoformat(record['timestamp'])
                    metrics = record['metrics']
                    
                    self.performance_history.append({
                        'timestamp': timestamp,
                        'metrics': metrics
                    })
                    
                    # æ›´æ–°è¶‹åŠ¿æ•°æ®
                    for metric_name, value in metrics.items():
                        self.metric_trends[metric_name].append({
                            'timestamp': timestamp,
                            'value': value
                        })
                
                logger.info(f"åŠ è½½äº† {len(self.performance_history)} æ¡å†å²è®°å½•")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ€§èƒ½å†å²æ•°æ®å¤±è´¥: {e}")
    
    def _save_performance_history(self):
        """ä¿å­˜æ€§èƒ½å†å²æ•°æ®"""
        try:
            history_file = self.data_dir / "performance_history.json"
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            history_data = []
            for record in list(self.performance_history)[-1000:]:  # åªä¿å­˜æœ€è¿‘1000æ¡
                history_data.append({
                    'timestamp': record['timestamp'].isoformat(),
                    'metrics': record['metrics']
                })
            
            data = {
                'history': history_data,
                'last_updated': datetime.now().isoformat()
            }
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info("æ€§èƒ½å†å²æ•°æ®å·²ä¿å­˜")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ€§èƒ½å†å²æ•°æ®å¤±è´¥: {e}")
    
    def collect_current_metrics(self) -> Dict[str, PerformanceMetric]:
        """æ”¶é›†å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        try:
            current_time = datetime.now()
            metrics = {}
            
            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = self.cache_service.get_cache_statistics()
            
            # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
            metrics['cache_hit_rate'] = PerformanceMetric(
                name='cache_hit_rate',
                value=cache_stats.get('hit_rate', 0),
                timestamp=current_time,
                unit='%',
                description='ç¼“å­˜å‘½ä¸­ç‡'
            )
            
            metrics['redis_availability'] = PerformanceMetric(
                name='redis_availability',
                value=100.0 if cache_stats.get('redis_healthy', False) else 0.0,
                timestamp=current_time,
                unit='%',
                description='Rediså¯ç”¨æ€§'
            )
            
            metrics['system_health_score'] = PerformanceMetric(
                name='system_health_score',
                value=self._calculate_health_score(cache_stats),
                timestamp=current_time,
                unit='åˆ†',
                description='ç³»ç»Ÿå¥åº·è¯„åˆ†'
            )
            
            metrics['error_rate'] = PerformanceMetric(
                name='error_rate',
                value=self._calculate_error_rate(cache_stats),
                timestamp=current_time,
                unit='%',
                description='é”™è¯¯ç‡'
            )
            
            # æ¨¡æ‹Ÿä¸€äº›å…¶ä»–æŒ‡æ ‡ï¼ˆå®é™…åº”è¯¥ä»çœŸå®æ•°æ®æºè·å–ï¼‰
            metrics['cache_response_time'] = PerformanceMetric(
                name='cache_response_time',
                value=self._estimate_cache_response_time(),
                timestamp=current_time,
                unit='ms',
                description='ç¼“å­˜å“åº”æ—¶é—´'
            )
            
            metrics['api_response_time'] = PerformanceMetric(
                name='api_response_time',
                value=self._estimate_api_response_time(),
                timestamp=current_time,
                unit='ms',
                description='APIå“åº”æ—¶é—´'
            )
            
            metrics['memory_usage'] = PerformanceMetric(
                name='memory_usage',
                value=self._estimate_memory_usage(cache_stats),
                timestamp=current_time,
                unit='MB',
                description='å†…å­˜ä½¿ç”¨é‡'
            )
            
            # æ›´æ–°å†å²è®°å½•
            metric_values = {name: metric.value for name, metric in metrics.items()}
            self.performance_history.append({
                'timestamp': current_time,
                'metrics': metric_values
            })
            
            # æ›´æ–°è¶‹åŠ¿æ•°æ®
            for name, metric in metrics.items():
                self.metric_trends[name].append({
                    'timestamp': current_time,
                    'value': metric.value
                })
            
            return metrics
            
        except Exception as e:
            logger.error(f"æ”¶é›†å½“å‰æŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    def _calculate_health_score(self, stats: Dict) -> float:
        """è®¡ç®—ç³»ç»Ÿå¥åº·è¯„åˆ†"""
        try:
            score = 100.0
            
            # ç¼“å­˜å‘½ä¸­ç‡å½±å“
            hit_rate = stats.get('hit_rate', 0)
            if hit_rate < 50:
                score -= 40
            elif hit_rate < 70:
                score -= 20
            elif hit_rate < 85:
                score -= 10
            
            # Rediså¯ç”¨æ€§å½±å“
            if not stats.get('redis_healthy', False):
                score -= 30
            
            # é”™è¯¯ç‡å½±å“
            error_rate = self._calculate_error_rate(stats)
            if error_rate > 15:
                score -= 20
            elif error_rate > 5:
                score -= 10
            
            return max(0, score)
            
        except Exception as e:
            logger.error(f"è®¡ç®—å¥åº·è¯„åˆ†å¤±è´¥: {e}")
            return 50.0
    
    def _calculate_error_rate(self, stats: Dict) -> float:
        """è®¡ç®—é”™è¯¯ç‡"""
        try:
            errors = stats.get('errors', 0)
            total = stats.get('total_requests', 1)
            return (errors / max(total, 1)) * 100
        except:
            return 0.0
    
    def _estimate_cache_response_time(self) -> float:
        """ä¼°ç®—ç¼“å­˜å“åº”æ—¶é—´"""
        # å®é™…åº”è¯¥ä»ç›‘æ§ç³»ç»Ÿè·å–çœŸå®æ•°æ®
        # è¿™é‡Œæä¾›ä¸€ä¸ªç®€å•çš„ä¼°ç®—
        try:
            cache_stats = self.cache_service.get_cache_statistics()
            hit_rate = cache_stats.get('hit_rate', 85)
            
            # å‘½ä¸­ç‡è¶Šé«˜ï¼Œå“åº”æ—¶é—´è¶Šä½
            if hit_rate > 90:
                return 5.0  # 5ms
            elif hit_rate > 80:
                return 8.0  # 8ms
            elif hit_rate > 60:
                return 15.0  # 15ms
            else:
                return 25.0  # 25ms
        except:
            return 10.0
    
    def _estimate_api_response_time(self) -> float:
        """ä¼°ç®—APIå“åº”æ—¶é—´"""
        # å®é™…åº”è¯¥ä»APIç›‘æ§è·å–çœŸå®æ•°æ®
        try:
            cache_stats = self.cache_service.get_cache_statistics()
            hit_rate = cache_stats.get('hit_rate', 85)
            redis_healthy = cache_stats.get('redis_healthy', True)
            
            base_time = 100.0  # åŸºç¡€å“åº”æ—¶é—´100ms
            
            # ç¼“å­˜å‘½ä¸­ç‡å½±å“
            if hit_rate > 90:
                base_time *= 0.5
            elif hit_rate > 70:
                base_time *= 0.7
            elif hit_rate < 50:
                base_time *= 1.5
            
            # Rediså¯ç”¨æ€§å½±å“
            if not redis_healthy:
                base_time *= 1.3
            
            return base_time
        except:
            return 150.0
    
    def _estimate_memory_usage(self, stats: Dict) -> float:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡"""
        try:
            # åŸºäºç¼“å­˜ç»Ÿè®¡ä¼°ç®—å†…å­˜ä½¿ç”¨
            fallback_size = stats.get('memory_fallback_size', 0)
            base_memory = 50.0  # åŸºç¡€å†…å­˜50MB
            
            # æ¯ä¸ªç¼“å­˜æ¡ç›®çº¦1KB
            cache_memory = fallback_size * 0.001  # è½¬æ¢ä¸ºMB
            
            return base_memory + cache_memory
        except:
            return 50.0
    
    def create_baseline(self, name: str, description: str = "") -> BaselineSnapshot:
        """åˆ›å»ºæ–°çš„æ€§èƒ½åŸºçº¿"""
        try:
            current_metrics = self.collect_current_metrics()
            
            baseline = BaselineSnapshot(
                name=name,
                created_at=datetime.now(),
                metrics=current_metrics,
                description=description,
                metadata={
                    'total_metrics': len(current_metrics),
                    'created_by': 'baseline_comparator',
                    'version': '2.0.0'
                }
            )
            
            self.baselines[name] = baseline
            self.current_baseline = baseline
            
            self._save_baselines()
            
            logger.info(f"åˆ›å»ºæ–°åŸºçº¿: {name}")
            return baseline
            
        except Exception as e:
            logger.error(f"åˆ›å»ºåŸºçº¿å¤±è´¥: {e}")
            raise
    
    def set_current_baseline(self, name: str):
        """è®¾ç½®å½“å‰åŸºçº¿"""
        if name in self.baselines:
            self.current_baseline = self.baselines[name]
            logger.info(f"åˆ‡æ¢å½“å‰åŸºçº¿ä¸º: {name}")
        else:
            raise ValueError(f"åŸºçº¿ä¸å­˜åœ¨: {name}")
    
    def compare_with_baseline(self, baseline_name: Optional[str] = None) -> Dict[str, ComparisonResult]:
        """ä¸åŸºçº¿å¯¹æ¯”"""
        try:
            # ç¡®å®šè¦å¯¹æ¯”çš„åŸºçº¿
            if baseline_name:
                if baseline_name not in self.baselines:
                    raise ValueError(f"åŸºçº¿ä¸å­˜åœ¨: {baseline_name}")
                baseline = self.baselines[baseline_name]
            else:
                if not self.current_baseline:
                    raise ValueError("æœªè®¾ç½®å½“å‰åŸºçº¿")
                baseline = self.current_baseline
            
            # æ”¶é›†å½“å‰æŒ‡æ ‡
            current_metrics = self.collect_current_metrics()
            
            # æ‰§è¡Œå¯¹æ¯”
            comparison_results = {}
            
            for metric_name, current_metric in current_metrics.items():
                if metric_name in baseline.metrics:
                    baseline_metric = baseline.metrics[metric_name]
                    result = self._compare_metric(baseline_metric, current_metric)
                    comparison_results[metric_name] = result
            
            return comparison_results
            
        except Exception as e:
            logger.error(f"åŸºçº¿å¯¹æ¯”å¤±è´¥: {e}")
            raise
    
    def _compare_metric(self, baseline_metric: PerformanceMetric, current_metric: PerformanceMetric) -> ComparisonResult:
        """å¯¹æ¯”å•ä¸ªæŒ‡æ ‡"""
        baseline_value = baseline_metric.value
        current_value = current_metric.value
        
        # è®¡ç®—åå·®
        if baseline_value == 0:
            deviation_percent = 0 if current_value == 0 else float('inf')
        else:
            deviation_percent = ((current_value - baseline_value) / baseline_value) * 100
        
        deviation_absolute = current_value - baseline_value
        
        # ç¡®å®šçŠ¶æ€å’Œæ˜¾è‘—æ€§
        metric_def = self.metric_definitions.get(current_metric.name, {})
        higher_is_better = metric_def.get('higher_is_better', True)
        
        # çŠ¶æ€åˆ¤æ–­
        if abs(deviation_percent) < 5:
            status = 'stable'
        elif (higher_is_better and deviation_percent > 0) or (not higher_is_better and deviation_percent < 0):
            status = 'improved'
        else:
            status = 'degraded'
        
        # æ˜¾è‘—æ€§åˆ¤æ–­
        abs_deviation = abs(deviation_percent)
        if abs_deviation < 5:
            significance = 'low'
        elif abs_deviation < 15:
            significance = 'medium'
        elif abs_deviation < 30:
            significance = 'high'
        else:
            significance = 'critical'
        
        return ComparisonResult(
            metric_name=current_metric.name,
            baseline_value=baseline_value,
            current_value=current_value,
            deviation_percent=deviation_percent,
            deviation_absolute=deviation_absolute,
            status=status,
            significance=significance
        )
    
    def get_trend_analysis(self, metric_name: str, days: int = 7) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡è¶‹åŠ¿åˆ†æ"""
        try:
            if metric_name not in self.metric_trends:
                return {'error': f'æŒ‡æ ‡ä¸å­˜åœ¨: {metric_name}'}
            
            trend_data = list(self.metric_trends[metric_name])
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´
            cutoff_time = datetime.now() - timedelta(days=days)
            recent_data = [
                point for point in trend_data
                if point['timestamp'] >= cutoff_time
            ]
            
            if len(recent_data) < 2:
                return {'error': 'æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿'}
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            values = [point['value'] for point in recent_data]
            
            analysis = {
                'metric_name': metric_name,
                'time_range_days': days,
                'data_points': len(recent_data),
                'statistics': {
                    'mean': statistics.mean(values),
                    'median': statistics.median(values),
                    'min': min(values),
                    'max': max(values),
                    'std_dev': statistics.stdev(values) if len(values) > 1 else 0
                },
                'trend': self._calculate_trend(recent_data),
                'volatility': self._calculate_volatility(values),
                'latest_value': values[-1],
                'change_from_start': values[-1] - values[0],
                'change_percent': ((values[-1] - values[0]) / values[0] * 100) if values[0] != 0 else 0
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"è¶‹åŠ¿åˆ†æå¤±è´¥ {metric_name}: {e}")
            return {'error': str(e)}
    
    def _calculate_trend(self, data_points: List[Dict]) -> str:
        """è®¡ç®—è¶‹åŠ¿æ–¹å‘"""
        if len(data_points) < 2:
            return 'insufficient_data'
        
        values = [point['value'] for point in data_points]
        
        # ç®€å•çº¿æ€§è¶‹åŠ¿è®¡ç®—
        x = list(range(len(values)))
        n = len(values)
        
        sum_x = sum(x)
        sum_y = sum(values)
        sum_xy = sum(x[i] * values[i] for i in range(n))
        sum_x2 = sum(x[i] ** 2 for i in range(n))
        
        # è®¡ç®—æ–œç‡
        if n * sum_x2 - sum_x ** 2 == 0:
            return 'stable'
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        
        if abs(slope) < 0.01:
            return 'stable'
        elif slope > 0:
            return 'increasing'
        else:
            return 'decreasing'
    
    def _calculate_volatility(self, values: List[float]) -> str:
        """è®¡ç®—æ³¢åŠ¨æ€§"""
        if len(values) < 2:
            return 'unknown'
        
        mean_val = statistics.mean(values)
        if mean_val == 0:
            return 'unknown'
        
        cv = statistics.stdev(values) / mean_val * 100  # å˜å¼‚ç³»æ•°
        
        if cv < 5:
            return 'low'
        elif cv < 15:
            return 'medium'
        elif cv < 30:
            return 'high'
        else:
            return 'very_high'
    
    def generate_comparison_report(self, baseline_name: Optional[str] = None) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š"""
        try:
            # æ‰§è¡Œå¯¹æ¯”
            comparison_results = self.compare_with_baseline(baseline_name)
            
            # è·å–åŸºçº¿ä¿¡æ¯
            baseline = self.current_baseline if not baseline_name else self.baselines[baseline_name]
            
            # æ±‡æ€»ç»Ÿè®¡
            total_metrics = len(comparison_results)
            improved_count = len([r for r in comparison_results.values() if r.status == 'improved'])
            degraded_count = len([r for r in comparison_results.values() if r.status == 'degraded'])
            stable_count = len([r for r in comparison_results.values() if r.status == 'stable'])
            
            critical_issues = [r for r in comparison_results.values() if r.significance == 'critical']
            high_issues = [r for r in comparison_results.values() if r.significance == 'high']
            
            # ç”ŸæˆæŠ¥å‘Š
            report = {
                'report_metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'baseline_name': baseline.name,
                    'baseline_created_at': baseline.created_at.isoformat(),
                    'baseline_description': baseline.description
                },
                'summary': {
                    'total_metrics': total_metrics,
                    'improved_metrics': improved_count,
                    'degraded_metrics': degraded_count,
                    'stable_metrics': stable_count,
                    'critical_issues': len(critical_issues),
                    'high_priority_issues': len(high_issues)
                },
                'overall_score': self._calculate_overall_score(comparison_results),
                'detailed_comparisons': {
                    name: result.to_dict() for name, result in comparison_results.items()
                },
                'critical_alerts': [
                    {
                        'metric': r.metric_name,
                        'baseline_value': r.baseline_value,
                        'current_value': r.current_value,
                        'deviation_percent': r.deviation_percent,
                        'recommendation': self._get_recommendation(r)
                    }
                    for r in critical_issues
                ],
                'recommendations': self._generate_recommendations(comparison_results)
            }
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šå¤±è´¥: {e}")
            raise
    
    def _calculate_overall_score(self, comparison_results: Dict[str, ComparisonResult]) -> float:
        """è®¡ç®—æ•´ä½“æ€§èƒ½è¯„åˆ†"""
        if not comparison_results:
            return 0.0
        
        total_score = 100.0
        penalty_weights = {
            'critical': 15,
            'high': 8,
            'medium': 3,
            'low': 1
        }
        
        for result in comparison_results.values():
            if result.status == 'degraded':
                penalty = penalty_weights.get(result.significance, 1)
                total_score -= penalty
        
        return max(0, total_score)
    
    def _get_recommendation(self, result: ComparisonResult) -> str:
        """è·å–æ”¹è¿›å»ºè®®"""
        metric_name = result.metric_name
        
        recommendations = {
            'cache_hit_rate': "æ£€æŸ¥ç¼“å­˜é…ç½®å’Œæ•°æ®è®¿é—®æ¨¡å¼ï¼Œè€ƒè™‘ä¼˜åŒ–ç¼“å­˜ç­–ç•¥",
            'cache_response_time': "æ£€æŸ¥Redisè¿æ¥çŠ¶æ€ï¼Œä¼˜åŒ–ç¼“å­˜æŸ¥è¯¢é€»è¾‘",
            'api_response_time': "æ£€æŸ¥APIæ€§èƒ½ç“¶é¢ˆï¼Œè€ƒè™‘å¢åŠ ç¼“å­˜æˆ–ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢",
            'system_health_score': "å…¨é¢æ£€æŸ¥ç³»ç»Ÿå„é¡¹æŒ‡æ ‡ï¼Œé‡ç‚¹å…³æ³¨é”™è¯¯ç‡å’Œå¯ç”¨æ€§",
            'error_rate': "æ£€æŸ¥åº”ç”¨æ—¥å¿—ï¼Œä¿®å¤é”™è¯¯æ ¹å› ï¼ŒåŠ å¼ºé”™è¯¯å¤„ç†",
            'redis_availability': "æ£€æŸ¥RedisæœåŠ¡çŠ¶æ€ï¼Œç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š",
            'memory_usage': "ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œæ¸…ç†ä¸å¿…è¦çš„ç¼“å­˜æ•°æ®"
        }
        
        return recommendations.get(metric_name, "æ£€æŸ¥è¯¥æŒ‡æ ‡çš„ç›¸å…³é…ç½®å’Œç³»ç»ŸçŠ¶æ€")
    
    def _generate_recommendations(self, comparison_results: Dict[str, ComparisonResult]) -> List[str]:
        """ç”Ÿæˆæ•´ä½“æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åˆ†æå…³é”®é—®é¢˜
        critical_issues = [r for r in comparison_results.values() if r.significance == 'critical']
        degraded_metrics = [r for r in comparison_results.values() if r.status == 'degraded']
        
        if critical_issues:
            recommendations.append("ğŸš¨ å‘ç°ä¸¥é‡æ€§èƒ½é€€åŒ–ï¼Œéœ€è¦ç«‹å³å¤„ç†å…³é”®é—®é¢˜")
        
        if len(degraded_metrics) > len(comparison_results) / 2:
            recommendations.append("âš ï¸ å¤šé¡¹æŒ‡æ ‡å‡ºç°é€€åŒ–ï¼Œå»ºè®®å…¨é¢æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
        
        # å…·ä½“å»ºè®®
        cache_issues = [r for r in comparison_results.values() 
                       if 'cache' in r.metric_name and r.status == 'degraded']
        if cache_issues:
            recommendations.append("ğŸ”§ ç¼“å­˜æ€§èƒ½ä¸‹é™ï¼Œæ£€æŸ¥Redisé…ç½®å’Œç½‘ç»œè¿æ¥")
        
        api_issues = [r for r in comparison_results.values() 
                     if 'api' in r.metric_name and r.status == 'degraded']
        if api_issues:
            recommendations.append("ğŸ”§ APIæ€§èƒ½ä¸‹é™ï¼Œæ£€æŸ¥åº”ç”¨æœåŠ¡å™¨å’Œæ•°æ®åº“æ€§èƒ½")
        
        error_issues = [r for r in comparison_results.values() 
                       if 'error' in r.metric_name and r.status == 'degraded']
        if error_issues:
            recommendations.append("ğŸ”§ é”™è¯¯ç‡ä¸Šå‡ï¼Œæ£€æŸ¥åº”ç”¨æ—¥å¿—å’Œå¼‚å¸¸å¤„ç†")
        
        if not recommendations:
            recommendations.append("âœ… ç³»ç»Ÿæ€§èƒ½ç¨³å®šï¼Œç»§ç»­ä¿æŒå½“å‰é…ç½®")
        
        return recommendations
    
    def export_baseline(self, baseline_name: str, file_path: str):
        """å¯¼å‡ºåŸºçº¿æ•°æ®"""
        try:
            if baseline_name not in self.baselines:
                raise ValueError(f"åŸºçº¿ä¸å­˜åœ¨: {baseline_name}")
            
            baseline = self.baselines[baseline_name]
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(baseline.to_dict(), f, ensure_ascii=False, indent=2)
            
            logger.info(f"åŸºçº¿å·²å¯¼å‡ºåˆ°: {file_path}")
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºåŸºçº¿å¤±è´¥: {e}")
            raise
    
    def import_baseline(self, file_path: str, baseline_name: Optional[str] = None):
        """å¯¼å…¥åŸºçº¿æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            baseline = BaselineSnapshot.from_dict(data)
            
            if baseline_name:
                baseline.name = baseline_name
            
            self.baselines[baseline.name] = baseline
            self._save_baselines()
            
            logger.info(f"åŸºçº¿å·²å¯¼å…¥: {baseline.name}")
            
        except Exception as e:
            logger.error(f"å¯¼å…¥åŸºçº¿å¤±è´¥: {e}")
            raise
    
    def get_baseline_list(self) -> List[Dict[str, Any]]:
        """è·å–åŸºçº¿åˆ—è¡¨"""
        return [
            {
                'name': baseline.name,
                'created_at': baseline.created_at.isoformat(),
                'description': baseline.description,
                'metrics_count': len(baseline.metrics),
                'is_current': baseline == self.current_baseline
            }
            for baseline in self.baselines.values()
        ]
    
    def delete_baseline(self, baseline_name: str):
        """åˆ é™¤åŸºçº¿"""
        if baseline_name not in self.baselines:
            raise ValueError(f"åŸºçº¿ä¸å­˜åœ¨: {baseline_name}")
        
        if self.current_baseline and self.current_baseline.name == baseline_name:
            self.current_baseline = None
        
        del self.baselines[baseline_name]
        self._save_baselines()
        
        logger.info(f"åŸºçº¿å·²åˆ é™¤: {baseline_name}")
    
    def cleanup_old_data(self, days: int = 30):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        try:
            cutoff_time = datetime.now() - timedelta(days=days)
            
            # æ¸…ç†æ€§èƒ½å†å²
            old_count = len(self.performance_history)
            self.performance_history = deque(
                [record for record in self.performance_history 
                 if record['timestamp'] >= cutoff_time],
                maxlen=10000
            )
            new_count = len(self.performance_history)
            
            # æ¸…ç†è¶‹åŠ¿æ•°æ®
            for metric_name in self.metric_trends:
                self.metric_trends[metric_name] = deque(
                    [point for point in self.metric_trends[metric_name]
                     if point['timestamp'] >= cutoff_time],
                    maxlen=1000
                )
            
            self._save_performance_history()
            
            logger.info(f"æ¸…ç†äº† {old_count - new_count} æ¡è¿‡æœŸè®°å½•")
            
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")


# å…¨å±€åŸºçº¿å¯¹æ¯”å™¨å®ä¾‹
_baseline_comparator = None
_comparator_lock = threading.Lock()

def get_baseline_comparator() -> LLMBaselineComparator:
    """è·å–åŸºçº¿å¯¹æ¯”å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _baseline_comparator
    
    if _baseline_comparator is None:
        with _comparator_lock:
            if _baseline_comparator is None:
                _baseline_comparator = LLMBaselineComparator()
    
    return _baseline_comparator

if __name__ == "__main__":
    """æµ‹è¯•åŸºçº¿å¯¹æ¯”å™¨"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LLMæ€§èƒ½åŸºçº¿å¯¹æ¯”")
    parser.add_argument("--create-baseline", type=str, help="åˆ›å»ºæ–°åŸºçº¿")
    parser.add_argument("--compare", type=str, help="ä¸æŒ‡å®šåŸºçº¿å¯¹æ¯”")
    parser.add_argument("--trend", type=str, help="æŸ¥çœ‹æŒ‡æ ‡è¶‹åŠ¿")
    parser.add_argument("--report", action="store_true", help="ç”Ÿæˆå®Œæ•´å¯¹æ¯”æŠ¥å‘Š")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰åŸºçº¿")
    parser.add_argument("--export", nargs=2, metavar=('baseline', 'file'), help="å¯¼å‡ºåŸºçº¿")
    parser.add_argument("--import", nargs=1, metavar='file', help="å¯¼å…¥åŸºçº¿")
    parser.add_argument("--cleanup", type=int, default=30, help="æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„æ•°æ®")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    comparator = get_baseline_comparator()
    
    if args.create_baseline:
        baseline = comparator.create_baseline(
            name=args.create_baseline,
            description=f"é€šè¿‡å‘½ä»¤è¡Œåˆ›å»ºçš„åŸºçº¿ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )
        print(f"åŸºçº¿å·²åˆ›å»º: {baseline.name}")
        print(f"åŒ…å«æŒ‡æ ‡: {len(baseline.metrics)} ä¸ª")
    
    elif args.compare:
        try:
            results = comparator.compare_with_baseline(args.compare)
            print(f"\nä¸åŸºçº¿ '{args.compare}' çš„å¯¹æ¯”ç»“æœ:")
            print("-" * 50)
            
            for metric_name, result in results.items():
                status_icon = "âœ…" if result.status == "improved" else "âš ï¸" if result.status == "stable" else "âŒ"
                print(f"{status_icon} {metric_name}: {result.deviation_percent:+.2f}% ({result.significance})")
        
        except Exception as e:
            print(f"å¯¹æ¯”å¤±è´¥: {e}")
    
    elif args.trend:
        analysis = comparator.get_trend_analysis(args.trend)
        if 'error' not in analysis:
            print(f"\næŒ‡æ ‡ '{args.trend}' çš„è¶‹åŠ¿åˆ†æ:")
            print("-" * 50)
            print(f"æ•°æ®ç‚¹æ•°: {analysis['data_points']}")
            print(f"è¶‹åŠ¿æ–¹å‘: {analysis['trend']}")
            print(f"æ³¢åŠ¨æ€§: {analysis['volatility']}")
            print(f"æœ€æ–°å€¼: {analysis['latest_value']:.2f}")
            print(f"å˜åŒ–å¹…åº¦: {analysis['change_percent']:+.2f}%")
        else:
            print(f"åˆ†æå¤±è´¥: {analysis['error']}")
    
    elif args.report:
        try:
            report = comparator.generate_comparison_report()
            print("\næ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
            print("=" * 50)
            print(f"åŸºçº¿: {report['report_metadata']['baseline_name']}")
            print(f"ç”Ÿæˆæ—¶é—´: {report['report_metadata']['generated_at']}")
            print(f"æ•´ä½“è¯„åˆ†: {report['overall_score']:.1f}/100")
            print(f"\næŒ‡æ ‡æ±‡æ€»:")
            print(f"  æ”¹å–„: {report['summary']['improved_metrics']} é¡¹")
            print(f"  ç¨³å®š: {report['summary']['stable_metrics']} é¡¹")
            print(f"  é€€åŒ–: {report['summary']['degraded_metrics']} é¡¹")
            print(f"  å…³é”®é—®é¢˜: {report['summary']['critical_issues']} é¡¹")
            
            if report['critical_alerts']:
                print(f"\nå…³é”®å‘Šè­¦:")
                for alert in report['critical_alerts']:
                    print(f"  âŒ {alert['metric']}: {alert['deviation_percent']:+.2f}%")
            
            print(f"\næ”¹è¿›å»ºè®®:")
            for rec in report['recommendations']:
                print(f"  â€¢ {rec}")
        
        except Exception as e:
            print(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
    
    elif getattr(args, 'list', False):
        baselines = comparator.get_baseline_list()
        print("\nå¯ç”¨åŸºçº¿:")
        print("-" * 50)
        for baseline in baselines:
            current_mark = " (å½“å‰)" if baseline['is_current'] else ""
            print(f"â€¢ {baseline['name']}{current_mark}")
            print(f"  åˆ›å»ºæ—¶é—´: {baseline['created_at']}")
            print(f"  æŒ‡æ ‡æ•°é‡: {baseline['metrics_count']}")
            if baseline['description']:
                print(f"  æè¿°: {baseline['description']}")
            print()
    
    elif args.export:
        baseline_name, file_path = args.export
        try:
            comparator.export_baseline(baseline_name, file_path)
            print(f"åŸºçº¿ '{baseline_name}' å·²å¯¼å‡ºåˆ° '{file_path}'")
        except Exception as e:
            print(f"å¯¼å‡ºå¤±è´¥: {e}")
    
    elif getattr(args, 'import', None):
        file_path = getattr(args, 'import')[0]
        try:
            comparator.import_baseline(file_path)
            print(f"åŸºçº¿å·²ä» '{file_path}' å¯¼å…¥")
        except Exception as e:
            print(f"å¯¼å…¥å¤±è´¥: {e}")
    
    elif args.cleanup:
        comparator.cleanup_old_data(args.cleanup)
        print(f"å·²æ¸…ç† {args.cleanup} å¤©å‰çš„æ•°æ®")
    
    else:
        print("è¯·æŒ‡å®šæ“ä½œå‚æ•°ï¼Œä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©")
