# project/routers/ai/ai_core.py
"""
AIæ¨¡å—ä¼˜åŒ–ç‰ˆæœ¬ - ä¸“é¡¹AIåŠŸèƒ½ä¼˜åŒ–
åŸºäºæˆåŠŸä¼˜åŒ–æ¨¡å¼ï¼Œä¸“é—¨ä¼˜åŒ–AIæ¨¡å—çš„æ ¸å¿ƒåŠŸèƒ½
"""
import asyncio
from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks, Query, Form
from sqlalchemy.orm import Session
from typing import List, Optional, Dict, Any, Union, Literal
from datetime import datetime
import logging
import time

# æ ¸å¿ƒä¾èµ–
from project.database import get_db
from project.utils import get_current_user_id
import project.schemas as schemas
from pydantic import BaseModel, Field, validator, ConfigDict

# ä¼˜åŒ–å·¥å…·å¯¼å…¥
from project.services.ai_service import (
    AIConversationService, AIMessageService, AIChatService, 
    AISemanticSearchService, AIUtilities
)
from project.utils.core.error_decorators import handle_database_errors, database_transaction
from project.utils.optimization.router_optimization import optimized_route, router_optimizer
from project.utils.async_cache.async_tasks import submit_background_task, TaskPriority
from project.utils.optimization.production_utils import cache_manager

# å¯¼å…¥AIé…ç½®
try:
    from project.ai_providers.ai_config import EnterpriseAIRouterConfig
    config = EnterpriseAIRouterConfig()
except ImportError:
    config = None

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/ai", tags=["AIæ™ºèƒ½æœåŠ¡"])

# ===== è¯·æ±‚/å“åº”æ¨¡å‹ =====

class ChatRequest(BaseModel):
    """ä¼˜åŒ–çš„èŠå¤©è¯·æ±‚æ¨¡å‹"""
    model_config = ConfigDict(protected_namespaces=())
    
    message: str = Field(..., max_length=10000, description="ç”¨æˆ·æ¶ˆæ¯å†…å®¹")
    conversation_id: Optional[int] = Field(None, description="å¯¹è¯ID")
    model_preference: Optional[str] = Field(None, description="åå¥½æ¨¡å‹")
    temperature: float = Field(default=0.7, ge=0, le=2, description="ç”Ÿæˆæ¸©åº¦")
    max_tokens: Optional[int] = Field(default=None, gt=0, le=8192, description="æœ€å¤§tokenæ•°")
    tools_enabled: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨å·¥å…·")
    rag_enabled: bool = Field(default=True, description="æ˜¯å¦å¯ç”¨RAGæ£€ç´¢")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼å“åº”")
    
    @validator('message')
    def validate_message(cls, v):
        if not v.strip():
            raise ValueError('æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º')
        return v.strip()

class ChatResponse(BaseModel):
    """ä¼˜åŒ–çš„èŠå¤©å“åº”æ¨¡å‹"""
    model_config = ConfigDict(protected_namespaces=())
    
    conversation_id: int
    user_message_id: int
    ai_message_id: int
    content: str
    model_used: str
    tokens_used: int
    response_time_ms: float
    tools_used: List[str] = []
    cached: bool = False

class SemanticSearchRequest(BaseModel):
    """è¯­ä¹‰æœç´¢è¯·æ±‚æ¨¡å‹"""
    query: str = Field(..., min_length=1, max_length=1000)
    item_types: Optional[List[str]] = Field(default=None)
    limit: int = Field(default=10, ge=1, le=50)

# ===== AIèŠå¤©è·¯ç”± =====

@router.post("/chat", response_model=ChatResponse, summary="AIæ™ºèƒ½å¯¹è¯")
@optimized_route("AIèŠå¤©")
@handle_database_errors
async def chat_with_ai(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """AIæ™ºèƒ½å¯¹è¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    start_time = time.time()
    
    # éªŒè¯è¯·æ±‚æ•°æ®
    chat_data = AIUtilities.validate_chat_request(request.dict())
    
    # æ„å»ºèŠå¤©é€‰é¡¹
    options = {
        "model_preference": request.model_preference,
        "temperature": request.temperature,
        "max_tokens": request.max_tokens,
        "tools_enabled": request.tools_enabled,
        "rag_enabled": request.rag_enabled,
        "stream": request.stream
    }
    
    # ä½¿ç”¨äº‹åŠ¡å¤„ç†èŠå¤©
    with database_transaction(db):
        result = await AIChatService.process_chat_optimized(
            db, current_user_id, request.message, 
            request.conversation_id, options
        )
        
        # å¼‚æ­¥å¤„ç†èŠå¤©åä»»åŠ¡
        submit_background_task(
            background_tasks,
            "process_ai_chat_analytics",
            {
                "user_id": current_user_id,
                "conversation_id": result["conversation_id"],
                "message_length": len(request.message),
                "tools_used": result["metadata"].get("tools_used", [])
            },
            priority=TaskPriority.LOW
        )
    
    response_time = (time.time() - start_time) * 1000
    
    logger.info(f"ç”¨æˆ· {current_user_id} AIèŠå¤©å®Œæˆï¼Œè€—æ—¶ {response_time:.2f}ms")
    
    return ChatResponse(
        conversation_id=result["conversation_id"],
        user_message_id=result["user_message_id"],
        ai_message_id=result["ai_message_id"],
        content=result["response"],
        model_used=result["metadata"].get("model_used", "unknown"),
        tokens_used=result["metadata"].get("tokens_used", 0),
        response_time_ms=response_time,
        tools_used=result["metadata"].get("tools_used", []),
        cached=result["metadata"].get("cached", False)
    )

@router.post("/chat/stream", summary="æµå¼AIå¯¹è¯")
@optimized_route("æµå¼AIèŠå¤©")
@handle_database_errors
async def stream_chat_with_ai(
    request: ChatRequest,
    background_tasks: BackgroundTasks,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """æµå¼AIå¯¹è¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    # å¼ºåˆ¶å¯ç”¨æµå¼æ¨¡å¼
    request.stream = True
    
    # å¤„ç†ä¸æ™®é€šèŠå¤©ç›¸åŒï¼Œä½†è¿”å›æµå¼å“åº”
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è¿”å› StreamingResponse
    result = await chat_with_ai(request, background_tasks, current_user_id, db)
    
    return {"message": "æµå¼å“åº”åŠŸèƒ½å¼€å‘ä¸­", "fallback_result": result}

# ===== å¯¹è¯ç®¡ç†è·¯ç”± =====

@router.get("/conversations", response_model=List[schemas.AIConversationResponse], summary="è·å–å¯¹è¯åˆ—è¡¨")
@optimized_route("è·å–å¯¹è¯åˆ—è¡¨")
@handle_database_errors
async def get_conversations(
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """è·å–ç”¨æˆ·çš„AIå¯¹è¯åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    conversations, total = AIConversationService.get_conversations_optimized(
        db, current_user_id, limit, offset
    )
    
    return [AIUtilities.format_conversation_response(conv) for conv in conversations]

@router.post("/conversations", response_model=schemas.AIConversationResponse, summary="åˆ›å»ºæ–°å¯¹è¯")
@optimized_route("åˆ›å»ºå¯¹è¯")
@handle_database_errors
async def create_conversation(
    title: Optional[str] = Form(None),
    initial_message: Optional[str] = Form(None),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """åˆ›å»ºæ–°å¯¹è¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    with database_transaction(db):
        conversation = AIConversationService.create_conversation_optimized(
            db, current_user_id, title, initial_message
        )
    
    logger.info(f"ç”¨æˆ· {current_user_id} åˆ›å»ºå¯¹è¯ {conversation.id}")
    return AIUtilities.format_conversation_response(conversation)

@router.get("/conversations/{conversation_id}", response_model=schemas.AIConversationResponse, summary="è·å–å¯¹è¯è¯¦æƒ…")
@optimized_route("è·å–å¯¹è¯è¯¦æƒ…")
@handle_database_errors
async def get_conversation(
    conversation_id: int,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """è·å–å¯¹è¯è¯¦æƒ… - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    conversation = AIConversationService.get_conversation_optimized(
        db, conversation_id, current_user_id
    )
    
    return AIUtilities.format_conversation_response(conversation)

@router.put("/conversations/{conversation_id}", response_model=schemas.AIConversationResponse, summary="æ›´æ–°å¯¹è¯")
@optimized_route("æ›´æ–°å¯¹è¯")
@handle_database_errors
async def update_conversation(
    conversation_id: int,
    title: str = Form(...),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """æ›´æ–°å¯¹è¯ä¿¡æ¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    with database_transaction(db):
        conversation = AIConversationService.update_conversation_optimized(
            db, conversation_id, current_user_id, title
        )
    
    logger.info(f"ç”¨æˆ· {current_user_id} æ›´æ–°å¯¹è¯ {conversation_id}")
    return AIUtilities.format_conversation_response(conversation)

@router.delete("/conversations/{conversation_id}", status_code=status.HTTP_204_NO_CONTENT, summary="åˆ é™¤å¯¹è¯")
@optimized_route("åˆ é™¤å¯¹è¯")
@handle_database_errors
async def delete_conversation(
    conversation_id: int,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """åˆ é™¤å¯¹è¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    with database_transaction(db):
        AIConversationService.delete_conversation_optimized(
            db, conversation_id, current_user_id
        )
    
    logger.info(f"ç”¨æˆ· {current_user_id} åˆ é™¤å¯¹è¯ {conversation_id}")

# ===== æ¶ˆæ¯ç®¡ç†è·¯ç”± =====

@router.get("/conversations/{conversation_id}/messages", response_model=List[schemas.AIConversationMessageResponse], summary="è·å–å¯¹è¯æ¶ˆæ¯")
@optimized_route("è·å–å¯¹è¯æ¶ˆæ¯")
@handle_database_errors
async def get_conversation_messages(
    conversation_id: int,
    limit: int = Query(50, ge=1, le=100),
    offset: int = Query(0, ge=0),
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """è·å–å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    messages, total = AIMessageService.get_messages_optimized(
        db, conversation_id, current_user_id, limit, offset
    )
    
    return [AIUtilities.format_message_response(msg) for msg in messages]

@router.delete("/messages/{message_id}", status_code=status.HTTP_204_NO_CONTENT, summary="åˆ é™¤æ¶ˆæ¯")
@optimized_route("åˆ é™¤æ¶ˆæ¯")
@handle_database_errors
async def delete_message(
    message_id: int,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """åˆ é™¤æ¶ˆæ¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    with database_transaction(db):
        AIMessageService.delete_message_optimized(db, message_id, current_user_id)
    
    logger.info(f"ç”¨æˆ· {current_user_id} åˆ é™¤æ¶ˆæ¯ {message_id}")

# ===== è¯­ä¹‰æœç´¢è·¯ç”± =====

@router.post("/semantic-search", response_model=List[Dict[str, Any]], summary="è¯­ä¹‰æœç´¢")
@optimized_route("è¯­ä¹‰æœç´¢")
@handle_database_errors
async def semantic_search(
    request: SemanticSearchRequest,
    background_tasks: BackgroundTasks,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """æ™ºèƒ½è¯­ä¹‰æœç´¢ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    results = await AISemanticSearchService.semantic_search_optimized(
        db, current_user_id, request.query, request.item_types, request.limit
    )
    
    # å¼‚æ­¥è®°å½•æœç´¢æ—¥å¿—
    submit_background_task(
        background_tasks,
        "log_semantic_search",
        {
            "user_id": current_user_id,
            "query": request.query,
            "item_types": request.item_types,
            "result_count": len(results)
        },
        priority=TaskPriority.LOW
    )
    
    logger.info(f"ç”¨æˆ· {current_user_id} è¯­ä¹‰æœç´¢ '{request.query}'ï¼š{len(results)} ä¸ªç»“æœ")
    return results

# ===== AIé…ç½®å’ŒçŠ¶æ€è·¯ç”± =====

@router.get("/config", summary="è·å–AIé…ç½®")
@optimized_route("è·å–AIé…ç½®")
@handle_database_errors
async def get_ai_config(
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """è·å–ç”¨æˆ·AIé…ç½® - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    # è·å–ç”¨æˆ·AIé…ç½®
    from project.models import User
    user = db.query(User).filter(User.id == current_user_id).first()
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="ç”¨æˆ·ä¸å­˜åœ¨"
        )
    
    return {
        "has_api_key": bool(user.llm_api_key_encrypted),
        "api_type": user.llm_api_type,
        "model_id": user.llm_model_id,
        "model_ids": user.llm_model_ids or [],
        "api_base_url": user.llm_api_base_url
    }

@router.get("/stats", summary="è·å–AIä½¿ç”¨ç»Ÿè®¡")
@optimized_route("AIä½¿ç”¨ç»Ÿè®¡")
@handle_database_errors
async def get_ai_stats(
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db)
):
    """è·å–AIä½¿ç”¨ç»Ÿè®¡ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    stats = AIUtilities.get_user_ai_stats(db, current_user_id)
    return stats

@router.get("/health", summary="AIæœåŠ¡å¥åº·æ£€æŸ¥")
@optimized_route("AIå¥åº·æ£€æŸ¥")
@handle_database_errors
async def health_check():
    """AIæœåŠ¡å¥åº·æ£€æŸ¥ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    health_status = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "2.0.0",
        "environment": "production"
    }
    
    # æ£€æŸ¥AIæœåŠ¡ç»„ä»¶
    try:
        from project.ai_providers.agent_orchestrator import AgentOrchestrator
        health_status["ai_orchestrator"] = "available"
    except ImportError:
        health_status["ai_orchestrator"] = "unavailable"
    
    try:
        from project.ai_providers.embedding_provider import get_embeddings_from_api
        health_status["embedding_service"] = "available"
    except ImportError:
        health_status["embedding_service"] = "unavailable"
    
    return health_status

# ===== æ‰¹é‡æ“ä½œè·¯ç”± =====

@router.post("/conversations/batch-delete", summary="æ‰¹é‡åˆ é™¤å¯¹è¯")
@optimized_route("æ‰¹é‡åˆ é™¤å¯¹è¯")
@handle_database_errors
async def batch_delete_conversations(
    background_tasks: BackgroundTasks,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db),
    conversation_ids: List[int] = Form(...)
):
    """æ‰¹é‡åˆ é™¤å¯¹è¯ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    if not conversation_ids:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="è¯·æä¾›è¦åˆ é™¤çš„å¯¹è¯IDåˆ—è¡¨"
        )
    
    if len(conversation_ids) > 50:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="ä¸€æ¬¡æœ€å¤šåªèƒ½åˆ é™¤50ä¸ªå¯¹è¯"
        )
    
    deleted_count = 0
    
    with database_transaction(db):
        for conversation_id in conversation_ids:
            try:
                AIConversationService.delete_conversation_optimized(
                    db, conversation_id, current_user_id
                )
                deleted_count += 1
            except HTTPException:
                # è·³è¿‡ä¸å­˜åœ¨æˆ–æ— æƒé™çš„å¯¹è¯
                continue
        
        # å¼‚æ­¥è®°å½•æ‰¹é‡æ“ä½œæ—¥å¿—
        submit_background_task(
            background_tasks,
            "log_batch_operation",
            {
                "user_id": current_user_id,
                "operation": "batch_delete_conversations",
                "conversation_ids": conversation_ids,
                "success_count": deleted_count
            },
            priority=TaskPriority.LOW
        )
    
    logger.info(f"ç”¨æˆ· {current_user_id} æ‰¹é‡åˆ é™¤ {deleted_count} ä¸ªå¯¹è¯")
    return {
        "message": f"æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªå¯¹è¯",
        "deleted_count": deleted_count,
        "total_requested": len(conversation_ids)
    }

# ===== ç‰¹æ®ŠAIåŠŸèƒ½è·¯ç”± =====

@router.post("/summarize", summary="æ™ºèƒ½æ‘˜è¦")
@optimized_route("æ™ºèƒ½æ‘˜è¦")
@handle_database_errors
async def generate_summary(
    background_tasks: BackgroundTasks,
    current_user_id: int = Depends(get_current_user_id),
    db: Session = Depends(get_db),
    text: str = Form(..., max_length=20000),
    summary_type: str = Form("brief", regex="^(brief|detailed|key_points)$")
):
    """ç”Ÿæˆæ™ºèƒ½æ‘˜è¦ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    if len(text.strip()) < 100:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="æ–‡æœ¬å†…å®¹è‡³å°‘éœ€è¦100ä¸ªå­—ç¬¦"
        )
    
    # æ„å»ºæ‘˜è¦æç¤º
    prompts = {
        "brief": "è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆç®€è¦æ‘˜è¦ï¼š",
        "detailed": "è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆè¯¦ç»†æ‘˜è¦ï¼š",
        "key_points": "è¯·æå–ä»¥ä¸‹å†…å®¹çš„å…³é”®è¦ç‚¹ï¼š"
    }
    
    summary_prompt = f"{prompts[summary_type]}\n\n{text}"
    
    # ä½¿ç”¨èŠå¤©æœåŠ¡ç”Ÿæˆæ‘˜è¦
    options = {
        "temperature": 0.3,  # è¾ƒä½æ¸©åº¦ç¡®ä¿æ‘˜è¦å‡†ç¡®æ€§
        "max_tokens": 1000,
        "tools_enabled": False,  # æ‘˜è¦ä¸éœ€è¦å·¥å…·
        "rag_enabled": False    # æ‘˜è¦ä¸éœ€è¦RAG
    }
    
    with database_transaction(db):
        result = await AIChatService.process_chat_optimized(
            db, current_user_id, summary_prompt, None, options
        )
        
        # å¼‚æ­¥è®°å½•æ‘˜è¦ä½¿ç”¨
        submit_background_task(
            background_tasks,
            "log_ai_summary_usage",
            {
                "user_id": current_user_id,
                "text_length": len(text),
                "summary_type": summary_type,
                "summary_length": len(result["response"])
            },
            priority=TaskPriority.LOW
        )
    
    logger.info(f"ç”¨æˆ· {current_user_id} ç”Ÿæˆ {summary_type} æ‘˜è¦")
    return {
        "summary": result["response"],
        "summary_type": summary_type,
        "original_length": len(text),
        "summary_length": len(result["response"]),
        "compression_ratio": len(result["response"]) / len(text)
    }

# ä½¿ç”¨è·¯ç”±ä¼˜åŒ–å™¨åº”ç”¨æ‰¹é‡ä¼˜åŒ–
# # router_optimizer.apply_batch_optimizations(router, {
# #     "cache_ttl": 300,
# #     "enable_compression": True,
# #     "rate_limit": "200/minute",  # AIåŠŸèƒ½éœ€è¦æ›´é«˜é™é¢
# #     "monitoring": True
# # })

logger.info("ğŸ§  AI Core - AIæ ¸å¿ƒæ¨¡å—å·²åŠ è½½ (å…¨åŠŸèƒ½ç‰ˆæœ¬)")
